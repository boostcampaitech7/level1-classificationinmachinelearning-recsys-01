{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu121\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import add_dummy_feature\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# statsmodel\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from pmdarima.arima import auto_arima\n",
    "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf \n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.arima_model import ARIMAResults\n",
    "from statsmodels.multivariate.manova import MANOVA\n",
    "\n",
    "\n",
    "# model\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch.optim as optim\n",
    "from layers.transformer_encdec import Encoder, EncoderLayer\n",
    "from layers.selfattention_family import FullAttention, AttentionLayer\n",
    "from layers.embed import PatchEmbedding, DataEmbedding_wo_pos\n",
    "from layers.autoformer_encdec import series_decomp, series_decomp_multi\n",
    "from layers.autoformer_encdec import series_decomp_fixed, series_decomp_fixed_multi\n",
    "from layers.standard import Normalize\n",
    "\n",
    "print(torch.__version__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 호출\n",
    "data_path: str = \"../../../data\"\n",
    "train_df: pd.DataFrame = pd.read_csv(os.path.join(data_path, \"train.csv\")).assign(_type=\"train\") # train 에는 _type = train \n",
    "test_df: pd.DataFrame = pd.read_csv(os.path.join(data_path, \"test.csv\")).assign(_type=\"test\") # test 에는 _type = test\n",
    "submission_df: pd.DataFrame = pd.read_csv(os.path.join(data_path, \"test.csv\")) # ID, target 열만 가진 데이터 미리 호출\n",
    "df: pd.DataFrame = pd.concat([train_df, test_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:03<00:00, 30.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# HOURLY_ 로 시작하는 .csv 파일 이름을 file_names 에 할딩\n",
    "file_names: List[str] = [\n",
    "    f for f in os.listdir(data_path) if f.startswith(\"HOURLY_\") and f.endswith(\".csv\")\n",
    "]\n",
    "\n",
    "# 파일명 : 데이터프레임으로 딕셔너리 형태로 저장\n",
    "file_dict: Dict[str, pd.DataFrame] = {\n",
    "    f.replace(\".csv\", \"\"): pd.read_csv(os.path.join(data_path, f)) for f in file_names\n",
    "}\n",
    "\n",
    "for _file_name, _df in tqdm(file_dict.items()):\n",
    "    # 열 이름 중복 방지를 위해 {_file_name.lower()}_{col.lower()}로 변경, datetime 열을 ID로 변경\n",
    "    _rename_rule = {\n",
    "        col: f\"{_file_name.lower()}_{col.lower()}\" if col != \"datetime\" else \"ID\"\n",
    "        for col in _df.columns\n",
    "    }\n",
    "    _df = _df.rename(_rename_rule, axis=1)\n",
    "    df = df.merge(_df, on=\"ID\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing value check\n",
    "\n",
    "train_df = df.loc[df[\"_type\"] == \"train\"]\n",
    "# 각 열에서 누락된 값의 수 & 백분율 계산\n",
    "missing_values = train_df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(train_df)) * 100\n",
    "\n",
    "# 누락된 값 비율을 기준으로 열 정렬\n",
    "sorted_missing_percentage = missing_percentage.sort_values(ascending=False)\n",
    "\n",
    "# missing_value의 비율이 100%가 아닌 column만 추출\n",
    "non_missing_columns = sorted_missing_percentage[sorted_missing_percentage != 100.0].index.tolist()\n",
    "non_missing_columns.remove('ID')\n",
    "non_missing_columns.remove('target')\n",
    "non_missing_columns.remove('_type')\n",
    "\n",
    "new_data = train_df[['ID','target', '_type'] + non_missing_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이동평균으로 결측치 대체\n",
    "new_df_stab = new_data[non_missing_columns]\n",
    "\n",
    "# train\n",
    "window_size = 3\n",
    "new_df_stab = new_df_stab.apply(lambda col: col.fillna(col.rolling(window=window_size, min_periods=1).mean()))\n",
    "new_df_stab = new_df_stab.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "\n",
    "# 결측치 처리한 new_df 정의\n",
    "new_train_df = pd.concat([new_data[['ID','target','_type']], new_df_stab], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df.loc[df[\"_type\"] == \"test\"]\n",
    "new_test_df = test_df[['ID','target','_type'] + non_missing_columns]\n",
    "\n",
    "new_test_stab = new_test_df[non_missing_columns]\n",
    "# test\n",
    "window_size = 3\n",
    "new_test_stab = new_test_stab.apply(lambda col: col.fillna(col.rolling(window=window_size, min_periods=1).mean()))\n",
    "new_test_stab = new_test_stab.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "new_test_df = pd.concat([new_test_df[['ID','target','_type']], new_test_stab], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 비율을 계산\n",
    "missing_percentage = new_test_df.isnull().mean() * 100\n",
    "\n",
    "# 결측치 비율이 100%인 컬럼 이름만 출력\n",
    "columns_with_all_missing = missing_percentage[missing_percentage >= 50].index.tolist()\n",
    "\n",
    "# 100% 결측치가 있는 컬럼 출력\n",
    "columns_with_all_missing = [col for col in columns_with_all_missing if col not in ['target', 'hourly_market-data_price-ohlcv_all_exchange_spot_btc_usd_close']]\n",
    "\n",
    "# train_df와 test_df에서 columns_with_all_missing에 있는 컬럼 삭제\n",
    "new_train_df = new_train_df.drop(columns=columns_with_all_missing, errors='ignore')\n",
    "new_test_df = new_test_df.drop(columns=columns_with_all_missing, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이동평균을 기반으로 이상치를 처리하는 함수\n",
    "def replace_outlier(df, window=3, threshold=2):\n",
    "    df_cleaned = df.copy()\n",
    "    \n",
    "    # 숫자형 컬럼들에 대해 처리\n",
    "    for column in df_cleaned.select_dtypes(include=[np.number]).columns:\n",
    "        # 이동평균과 표준편차 계산\n",
    "        rolling_mean = df_cleaned[column].rolling(window=window, min_periods=1).mean()\n",
    "        rolling_std = df_cleaned[column].rolling(window=window, min_periods=1).std()\n",
    "\n",
    "        # 이상치 기준 설정\n",
    "        outliers = np.abs(df_cleaned[column] - rolling_mean) > (threshold * rolling_std)\n",
    "\n",
    "        # 이상치를 이동평균으로 대체\n",
    "        df_cleaned.loc[outliers, column] = rolling_mean[outliers]\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "# 이동평균 기반 이상치 처리 적용\n",
    "cleaned_train_df = replace_outlier(new_train_df)\n",
    "cleaned_test_df = replace_outlier(new_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardization(train_df, test_df):\n",
    "    features_to_scale = [col for col in train_df.columns if col not in ['ID', 'target', '_type']]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # 훈련 데이터 정규화\n",
    "    train_df_scaled = train_df.copy()\n",
    "    train_df_scaled[features_to_scale] = scaler.fit_transform(train_df[features_to_scale])\n",
    "\n",
    "    # 테스트 데이터 정규화\n",
    "    test_df_scaled = test_df.copy()\n",
    "    test_df_scaled[features_to_scale] = scaler.transform(test_df[features_to_scale])\n",
    "\n",
    "    return train_df_scaled, test_df_scaled\n",
    "\n",
    "# 함수 호출\n",
    "std_train_df, std_test_df = standardization(cleaned_train_df, cleaned_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Validation Set: 0.0006475219728316957\n"
     ]
    }
   ],
   "source": [
    "# 타겟과 피처 설정\n",
    "y_train = std_train_df['hourly_market-data_price-ohlcv_all_exchange_spot_btc_usd_close']\n",
    "X_train = std_train_df.drop(columns=['hourly_market-data_price-ohlcv_all_exchange_spot_btc_usd_close', 'ID', 'target', '_type'], errors='ignore')\n",
    "\n",
    "# 훈련 데이터와 검증 데이터 나누기\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# 모델 훈련\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# 검증 데이터에서 예측\n",
    "y_val_pred = model.predict(X_val)\n",
    "\n",
    "# 성능 평가\n",
    "mse = mean_squared_error(y_val, y_val_pred)\n",
    "print(\"Mean Squared Error on Validation Set:\", mse)\n",
    "\n",
    "# 모델 훈련\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# test_df에서 예측\n",
    "X_test = std_test_df.drop(columns=['hourly_market-data_price-ohlcv_all_exchange_spot_btc_usd_close', 'ID', 'target', '_type'], errors='ignore')\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# new_test_df에 y_pred 값을 추가\n",
    "std_test_df['hourly_market-data_price-ohlcv_all_exchange_spot_btc_usd_close'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "      <th>_type</th>\n",
       "      <th>hourly_market-data_funding-rates_bybit_funding_rates</th>\n",
       "      <th>hourly_market-data_taker-buy-sell-stats_bybit_taker_buy_volume</th>\n",
       "      <th>hourly_market-data_taker-buy-sell-stats_bybit_taker_sell_volume</th>\n",
       "      <th>hourly_market-data_taker-buy-sell-stats_bybit_taker_buy_ratio</th>\n",
       "      <th>hourly_market-data_taker-buy-sell-stats_bybit_taker_sell_ratio</th>\n",
       "      <th>hourly_market-data_taker-buy-sell-stats_bybit_taker_buy_sell_ratio</th>\n",
       "      <th>hourly_network-data_fees-transaction_fees_transaction_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>hourly_market-data_liquidations_binance_all_symbol_long_liquidations_usd</th>\n",
       "      <th>hourly_market-data_liquidations_binance_all_symbol_short_liquidations_usd</th>\n",
       "      <th>hourly_market-data_open-interest_htx_global_btc_usd_open_interest</th>\n",
       "      <th>hourly_network-data_addresses-count_addresses_count_receiver</th>\n",
       "      <th>hourly_network-data_fees_fees_total</th>\n",
       "      <th>hourly_network-data_fees_fees_total_usd</th>\n",
       "      <th>hourly_market-data_liquidations_htx_global_all_symbol_long_liquidations</th>\n",
       "      <th>hourly_market-data_liquidations_htx_global_all_symbol_short_liquidations</th>\n",
       "      <th>hourly_market-data_liquidations_htx_global_all_symbol_long_liquidations_usd</th>\n",
       "      <th>hourly_market-data_liquidations_htx_global_all_symbol_short_liquidations_usd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01 00:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>train</td>\n",
       "      <td>0.242277</td>\n",
       "      <td>-0.739005</td>\n",
       "      <td>-0.790786</td>\n",
       "      <td>0.677313</td>\n",
       "      <td>-0.660030</td>\n",
       "      <td>0.248331</td>\n",
       "      <td>-0.452496</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.327741</td>\n",
       "      <td>-0.346728</td>\n",
       "      <td>-0.099874</td>\n",
       "      <td>0.946969</td>\n",
       "      <td>-0.472265</td>\n",
       "      <td>-0.467622</td>\n",
       "      <td>-0.289134</td>\n",
       "      <td>-0.23536</td>\n",
       "      <td>-0.2937</td>\n",
       "      <td>-0.232519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-01 01:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>train</td>\n",
       "      <td>0.242277</td>\n",
       "      <td>-0.555248</td>\n",
       "      <td>-0.812900</td>\n",
       "      <td>2.230195</td>\n",
       "      <td>-2.214769</td>\n",
       "      <td>2.351257</td>\n",
       "      <td>-0.476448</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.328215</td>\n",
       "      <td>-0.319897</td>\n",
       "      <td>0.005954</td>\n",
       "      <td>-0.773384</td>\n",
       "      <td>-0.540988</td>\n",
       "      <td>-0.496686</td>\n",
       "      <td>-0.289134</td>\n",
       "      <td>-0.23536</td>\n",
       "      <td>-0.2937</td>\n",
       "      <td>-0.232519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-01 02:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>train</td>\n",
       "      <td>0.242277</td>\n",
       "      <td>-0.844533</td>\n",
       "      <td>-0.783052</td>\n",
       "      <td>-1.185413</td>\n",
       "      <td>1.204924</td>\n",
       "      <td>-0.646820</td>\n",
       "      <td>-0.417162</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.328215</td>\n",
       "      <td>-0.346728</td>\n",
       "      <td>-0.005201</td>\n",
       "      <td>-0.889786</td>\n",
       "      <td>-0.528197</td>\n",
       "      <td>-0.491262</td>\n",
       "      <td>-0.289134</td>\n",
       "      <td>-0.23536</td>\n",
       "      <td>-0.2937</td>\n",
       "      <td>-0.232519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-01 03:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>train</td>\n",
       "      <td>0.242277</td>\n",
       "      <td>-0.809874</td>\n",
       "      <td>-0.841254</td>\n",
       "      <td>0.671624</td>\n",
       "      <td>-0.654333</td>\n",
       "      <td>0.244119</td>\n",
       "      <td>-0.486115</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.304814</td>\n",
       "      <td>-0.346728</td>\n",
       "      <td>-0.036679</td>\n",
       "      <td>-0.447158</td>\n",
       "      <td>-0.549200</td>\n",
       "      <td>-0.500178</td>\n",
       "      <td>-0.289134</td>\n",
       "      <td>-0.23536</td>\n",
       "      <td>-0.2937</td>\n",
       "      <td>-0.232519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-01 04:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>train</td>\n",
       "      <td>0.242277</td>\n",
       "      <td>-0.701877</td>\n",
       "      <td>-0.660023</td>\n",
       "      <td>-0.369732</td>\n",
       "      <td>0.388268</td>\n",
       "      <td>-0.346476</td>\n",
       "      <td>-0.449115</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.313955</td>\n",
       "      <td>-0.346728</td>\n",
       "      <td>-0.036056</td>\n",
       "      <td>0.343477</td>\n",
       "      <td>-0.521007</td>\n",
       "      <td>-0.488245</td>\n",
       "      <td>-0.289134</td>\n",
       "      <td>-0.23536</td>\n",
       "      <td>-0.2937</td>\n",
       "      <td>-0.232519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 209 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ID  target  _type  \\\n",
       "0  2023-01-01 00:00:00     2.0  train   \n",
       "1  2023-01-01 01:00:00     1.0  train   \n",
       "2  2023-01-01 02:00:00     1.0  train   \n",
       "3  2023-01-01 03:00:00     1.0  train   \n",
       "4  2023-01-01 04:00:00     2.0  train   \n",
       "\n",
       "   hourly_market-data_funding-rates_bybit_funding_rates  \\\n",
       "0                                           0.242277      \n",
       "1                                           0.242277      \n",
       "2                                           0.242277      \n",
       "3                                           0.242277      \n",
       "4                                           0.242277      \n",
       "\n",
       "   hourly_market-data_taker-buy-sell-stats_bybit_taker_buy_volume  \\\n",
       "0                                          -0.739005                \n",
       "1                                          -0.555248                \n",
       "2                                          -0.844533                \n",
       "3                                          -0.809874                \n",
       "4                                          -0.701877                \n",
       "\n",
       "   hourly_market-data_taker-buy-sell-stats_bybit_taker_sell_volume  \\\n",
       "0                                          -0.790786                 \n",
       "1                                          -0.812900                 \n",
       "2                                          -0.783052                 \n",
       "3                                          -0.841254                 \n",
       "4                                          -0.660023                 \n",
       "\n",
       "   hourly_market-data_taker-buy-sell-stats_bybit_taker_buy_ratio  \\\n",
       "0                                           0.677313               \n",
       "1                                           2.230195               \n",
       "2                                          -1.185413               \n",
       "3                                           0.671624               \n",
       "4                                          -0.369732               \n",
       "\n",
       "   hourly_market-data_taker-buy-sell-stats_bybit_taker_sell_ratio  \\\n",
       "0                                          -0.660030                \n",
       "1                                          -2.214769                \n",
       "2                                           1.204924                \n",
       "3                                          -0.654333                \n",
       "4                                           0.388268                \n",
       "\n",
       "   hourly_market-data_taker-buy-sell-stats_bybit_taker_buy_sell_ratio  \\\n",
       "0                                           0.248331                    \n",
       "1                                           2.351257                    \n",
       "2                                          -0.646820                    \n",
       "3                                           0.244119                    \n",
       "4                                          -0.346476                    \n",
       "\n",
       "   hourly_network-data_fees-transaction_fees_transaction_mean  ...  \\\n",
       "0                                          -0.452496           ...   \n",
       "1                                          -0.476448           ...   \n",
       "2                                          -0.417162           ...   \n",
       "3                                          -0.486115           ...   \n",
       "4                                          -0.449115           ...   \n",
       "\n",
       "   hourly_market-data_liquidations_binance_all_symbol_long_liquidations_usd  \\\n",
       "0                                          -0.327741                          \n",
       "1                                          -0.328215                          \n",
       "2                                          -0.328215                          \n",
       "3                                          -0.304814                          \n",
       "4                                          -0.313955                          \n",
       "\n",
       "   hourly_market-data_liquidations_binance_all_symbol_short_liquidations_usd  \\\n",
       "0                                          -0.346728                           \n",
       "1                                          -0.319897                           \n",
       "2                                          -0.346728                           \n",
       "3                                          -0.346728                           \n",
       "4                                          -0.346728                           \n",
       "\n",
       "   hourly_market-data_open-interest_htx_global_btc_usd_open_interest  \\\n",
       "0                                          -0.099874                   \n",
       "1                                           0.005954                   \n",
       "2                                          -0.005201                   \n",
       "3                                          -0.036679                   \n",
       "4                                          -0.036056                   \n",
       "\n",
       "   hourly_network-data_addresses-count_addresses_count_receiver  \\\n",
       "0                                           0.946969              \n",
       "1                                          -0.773384              \n",
       "2                                          -0.889786              \n",
       "3                                          -0.447158              \n",
       "4                                           0.343477              \n",
       "\n",
       "   hourly_network-data_fees_fees_total  \\\n",
       "0                            -0.472265   \n",
       "1                            -0.540988   \n",
       "2                            -0.528197   \n",
       "3                            -0.549200   \n",
       "4                            -0.521007   \n",
       "\n",
       "   hourly_network-data_fees_fees_total_usd  \\\n",
       "0                                -0.467622   \n",
       "1                                -0.496686   \n",
       "2                                -0.491262   \n",
       "3                                -0.500178   \n",
       "4                                -0.488245   \n",
       "\n",
       "   hourly_market-data_liquidations_htx_global_all_symbol_long_liquidations  \\\n",
       "0                                          -0.289134                         \n",
       "1                                          -0.289134                         \n",
       "2                                          -0.289134                         \n",
       "3                                          -0.289134                         \n",
       "4                                          -0.289134                         \n",
       "\n",
       "   hourly_market-data_liquidations_htx_global_all_symbol_short_liquidations  \\\n",
       "0                                           -0.23536                          \n",
       "1                                           -0.23536                          \n",
       "2                                           -0.23536                          \n",
       "3                                           -0.23536                          \n",
       "4                                           -0.23536                          \n",
       "\n",
       "   hourly_market-data_liquidations_htx_global_all_symbol_long_liquidations_usd  \\\n",
       "0                                            -0.2937                             \n",
       "1                                            -0.2937                             \n",
       "2                                            -0.2937                             \n",
       "3                                            -0.2937                             \n",
       "4                                            -0.2937                             \n",
       "\n",
       "   hourly_market-data_liquidations_htx_global_all_symbol_short_liquidations_usd  \n",
       "0                                          -0.232519                             \n",
       "1                                          -0.232519                             \n",
       "2                                          -0.232519                             \n",
       "3                                          -0.232519                             \n",
       "4                                          -0.232519                             \n",
       "\n",
       "[5 rows x 209 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([std_train_df, std_test_df], ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11552, 20)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델에 사용할 컬럼, 컬럼의 rename rule을 미리 할당함\n",
    "cols_dict: Dict[str, str] = {\n",
    "    \"ID\": \"ID\",\n",
    "    \"target\": \"target\",\n",
    "    \"_type\": \"_type\",\n",
    "    \"hourly_market-data_coinbase-premium-index_coinbase_premium_gap\": \"coinbase_premium_gap\",\n",
    "    \"hourly_market-data_coinbase-premium-index_coinbase_premium_index\": \"coinbase_premium_index\",\n",
    "    \"hourly_market-data_funding-rates_all_exchange_funding_rates\": \"funding_rates\",\n",
    "    \"hourly_market-data_liquidations_all_exchange_all_symbol_long_liquidations\": \"long_liquidations\",\n",
    "    \"hourly_market-data_liquidations_all_exchange_all_symbol_long_liquidations_usd\": \"long_liquidations_usd\",\n",
    "    \"hourly_market-data_liquidations_all_exchange_all_symbol_short_liquidations\": \"short_liquidations\",\n",
    "    \"hourly_market-data_liquidations_all_exchange_all_symbol_short_liquidations_usd\": \"short_liquidations_usd\",\n",
    "    \"hourly_market-data_open-interest_all_exchange_all_symbol_open_interest\": \"open_interest\",\n",
    "    \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_buy_ratio\": \"buy_ratio\",\n",
    "    \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_buy_sell_ratio\": \"buy_sell_ratio\",\n",
    "    \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_buy_volume\": \"buy_volume\",\n",
    "    \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_sell_ratio\": \"sell_ratio\",\n",
    "    \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_sell_volume\": \"sell_volume\",\n",
    "    \"hourly_network-data_addresses-count_addresses_count_active\": \"active_count\",\n",
    "    \"hourly_network-data_addresses-count_addresses_count_receiver\": \"receiver_count\",\n",
    "    \"hourly_network-data_addresses-count_addresses_count_sender\": \"sender_count\",\n",
    "    \"hourly_market-data_price-ohlcv_all_exchange_spot_btc_usd_close\" : \"close\",\n",
    "}\n",
    "df = df[cols_dict.keys()].rename(cols_dict, axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda 에서 파악한 차이와 차이의 음수, 양수 여부를 새로운 피쳐로 생성\n",
    "df = df.assign(\n",
    "    liquidation_diff=df[\"long_liquidations\"] - df[\"short_liquidations\"],\n",
    "    liquidation_usd_diff=df[\"long_liquidations_usd\"] - df[\"short_liquidations_usd\"],\n",
    "    volume_diff=df[\"buy_volume\"] - df[\"sell_volume\"],\n",
    "    liquidation_diffg=np.sign(df[\"long_liquidations\"] - df[\"short_liquidations\"]),\n",
    "    liquidation_usd_diffg=np.sign(df[\"long_liquidations_usd\"] - df[\"short_liquidations_usd\"]),\n",
    "    volume_diffg=np.sign(df[\"buy_volume\"] - df[\"sell_volume\"]),\n",
    "    buy_sell_volume_ratio=df[\"buy_volume\"] / (df[\"sell_volume\"] + 1),\n",
    "    close_diff = df['close'].diff().fillna(0),\n",
    "    close_diffg = np.sign(df['close'].diff().fillna(0))\n",
    ")\n",
    "# category, continuous 열을 따로 할당해둠\n",
    "category_cols: List[str] = [\"liquidation_diffg\", \"liquidation_usd_diffg\", \"volume_diffg\", \"close_diffg\"]\n",
    "conti_cols: List[str] = [_ for _ in cols_dict.values() if _ not in [\"ID\", \"target\", \"_type\"]] + [\n",
    "    \"buy_sell_volume_ratio\",\n",
    "    \"liquidation_diff\",\n",
    "    \"liquidation_usd_diff\",\n",
    "    \"volume_diff\",\n",
    "    \"close_diff\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_feature(\n",
    "    df: pd.DataFrame,\n",
    "    conti_cols: List[str],\n",
    "    intervals: List[int],\n",
    ") -> List[pd.Series]:\n",
    "    \"\"\"\n",
    "    연속형 변수의 shift feature 생성\n",
    "    Args:\n",
    "        df (pd.DataFrame)\n",
    "        conti_cols (List[str]): continuous colnames\n",
    "        intervals (List[int]): shifted intervals\n",
    "    Return:\n",
    "        List[pd.Series]\n",
    "    \"\"\"\n",
    "    df_shift_dict = [\n",
    "        df[conti_col].shift(interval).rename(f\"{conti_col}_{interval}\")\n",
    "        for conti_col in conti_cols\n",
    "        for interval in intervals\n",
    "    ]\n",
    "    return df_shift_dict\n",
    "\n",
    "# 최대 24시간의 shift 피쳐를 계산\n",
    "shift_list = shift_feature(\n",
    "    df=df, conti_cols=conti_cols, intervals=[_ for _ in range(1, 24)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat 하여 df 에 할당\n",
    "df = pd.concat([df, pd.concat(shift_list, axis=1)], axis=1)\n",
    "\n",
    "# 타겟 변수를 제외한 변수를 forwardfill, -999로 결측치 대체\n",
    "_target = df[\"target\"]\n",
    "df = df.ffill().fillna(-999).assign(target = _target)\n",
    "\n",
    "# _type에 따라 train, test 분리\n",
    "train_df = df.loc[df[\"_type\"]==\"train\"].drop(columns=[\"_type\"])\n",
    "test_df = df.loc[df[\"_type\"]==\"test\"].drop(columns=[\"_type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### patchTST training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### data split & loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final data\n",
    "print(train_df.shape, test_df.shape)\n",
    "\n",
    "# train data split for training\n",
    "train_data = train_df.iloc[:,2:]\n",
    "valid_data = train_df[\"target\"]\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_data, valid_data, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "# 3. 데이터 텐서 변환\n",
    "train_X_tensor = torch.tensor(x_train.values, dtype=torch.float32)\n",
    "train_y_tensor = torch.tensor(y_train.values.squeeze(), dtype=torch.long)\n",
    "valid_X_tensor = torch.tensor(x_valid.values, dtype=torch.float32)\n",
    "valid_y_tensor = torch.tensor(y_valid.values.squeeze(), dtype=torch.long)\n",
    "\n",
    "# 4. 데이터셋과 DataLoader 준비\n",
    "# seq_len 설정\n",
    "seq_len = 24\n",
    "batch_size = 128\n",
    "\n",
    "# 5. 시계열 데이터에 맞게 3차원 텐서로 변환\n",
    "def create_sequences(X, y, seq_len):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(X) - seq_len):\n",
    "        seq = X[i:i + seq_len]\n",
    "        label = y[i + seq_len]  # 다음 시간 스텝의 레이블\n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "    return torch.stack(sequences), torch.tensor(labels)\n",
    "\n",
    "train_X_seq, train_y_seq = create_sequences(train_X_tensor, train_y_tensor, seq_len)\n",
    "valid_X_seq, valid_y_seq = create_sequences(valid_X_tensor, valid_y_tensor, seq_len)\n",
    "\n",
    "# DataLoader 설정\n",
    "train_dataset = TensorDataset(train_X_seq, train_y_seq)\n",
    "valid_dataset = TensorDataset(valid_X_seq, valid_y_seq)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 24, 532])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transpose(nn.Module):\n",
    "    def __init__(self, *dims, contiguous=False): \n",
    "        super().__init__()\n",
    "        self.dims, self.contiguous = dims, contiguous\n",
    "    def forward(self, x):\n",
    "        if self.contiguous: return x.transpose(*self.dims).contiguous()\n",
    "        else: return x.transpose(*self.dims)\n",
    "\n",
    "\n",
    "class FlattenHead(nn.Module):\n",
    "    def __init__(self, n_vars, nf, target_window, head_dropout=0):\n",
    "        super().__init__()\n",
    "        self.n_vars = n_vars\n",
    "        self.flatten = nn.Flatten(start_dim=-2)\n",
    "        self.linear = nn.Linear(nf, target_window)\n",
    "        self.dropout = nn.Dropout(head_dropout)\n",
    "\n",
    "    def forward(self, x):  # x: [bs x nvars x d_model x patch_num]\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, configs, patch_len=24, stride=12):\n",
    "        \"\"\"\n",
    "        patch_len: int, patch len for patch_embedding\n",
    "        stride: int, stride for patch_embedding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.task_name = configs['task_name']\n",
    "        self.seq_len = configs['seq_len']\n",
    "        self.pred_len = configs['pred_len']\n",
    "        padding = stride\n",
    "\n",
    "        # patching and embedding\n",
    "        self.patch_embedding = PatchEmbedding(\n",
    "            configs['d_model'], patch_len, stride, padding, configs['dropout'])\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(False, configs['factor'], attention_dropout=configs['dropout'],\n",
    "                                      output_attention=configs['output_attention']), configs['d_model'], configs['n_heads']),\n",
    "                    configs['d_model'],\n",
    "                    configs['d_ff'],\n",
    "                    dropout=configs['dropout'],\n",
    "                    activation=configs['activation']\n",
    "                ) for l in range(configs['e_layers'])\n",
    "            ],\n",
    "            norm_layer=nn.Sequential(Transpose(1,2), nn.BatchNorm1d(configs['d_model']), Transpose(1,2))\n",
    "        )\n",
    "\n",
    "        # Prediction Head\n",
    "        self.head_nf = configs['d_model'] * \\\n",
    "                       int((configs['seq_len'] - patch_len) / stride + 2)\n",
    "        if self.task_name == 'classification':\n",
    "            self.flatten = nn.Flatten(start_dim=-2)\n",
    "            self.dropout = nn.Dropout(configs['dropout'])\n",
    "            self.projection = nn.Linear(\n",
    "                self.head_nf * configs['enc_in'], configs['num_class'])\n",
    "\n",
    "    def classification(self, x_enc):\n",
    "        # Normalization from Non-stationary Transformer\n",
    "        means = x_enc.mean(1, keepdim=True).detach()\n",
    "        x_enc = x_enc - means\n",
    "        stdev = torch.sqrt(\n",
    "            torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
    "        x_enc /= stdev\n",
    "\n",
    "        # do patching and embedding\n",
    "        x_enc = x_enc.permute(0, 2, 1)\n",
    "        # u: [bs * nvars x patch_num x d_model]\n",
    "        enc_out, n_vars = self.patch_embedding(x_enc)\n",
    "\n",
    "        # Encoder\n",
    "        # z: [bs * nvars x patch_num x d_model]\n",
    "        enc_out, attns = self.encoder(enc_out)\n",
    "        # z: [bs x nvars x patch_num x d_model]\n",
    "        enc_out = torch.reshape(\n",
    "            enc_out, (-1, n_vars, enc_out.shape[-2], enc_out.shape[-1]))\n",
    "        # z: [bs x nvars x d_model x patch_num]\n",
    "        enc_out = enc_out.permute(0, 1, 3, 2)\n",
    "\n",
    "        # Decoder\n",
    "        output = self.flatten(enc_out)\n",
    "        output = self.dropout(output)\n",
    "        output = output.reshape(output.shape[0], -1)\n",
    "        output = self.projection(output)  # (batch_size, num_classes)\n",
    "        return output\n",
    "\n",
    "    def forward(self, x_enc, mask=None):\n",
    "        if self.task_name == 'classification':\n",
    "            dec_out = self.classification(x_enc)\n",
    "            return dec_out  # [B, N]\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    'task_name': 'classification',\n",
    "    'seq_len': 24,  # Total features\n",
    "    'pred_len': 24,  # Prediction length\n",
    "    'd_model': 128,  # Embedding dimension\n",
    "    'dropout': 0.1,\n",
    "    'factor': 5,\n",
    "    'output_attention': False,\n",
    "    'n_heads': 4,  # Number of heads in the Transformer\n",
    "    'e_layers': 3,  # Number of encoder layers\n",
    "    'd_ff': 256,  # Dimension of feed-forward layers\n",
    "    'enc_in': 532,  # Input size\n",
    "    'num_class': 4,  # Number of classes\n",
    "    'activation': 'relu'\n",
    "}\n",
    "\n",
    "model = Model(configs, patch_len=24, stride=12).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 3.7949223041534426\n",
      "Validation Accuracy: 43.11%\n",
      "Validation Loss: 1.7809\n",
      "Epoch [11/50], Loss: 1.1881092104044828\n",
      "Validation Accuracy: 39.41%\n",
      "Validation Loss: 2.7812\n",
      "Epoch [21/50], Loss: 1.0780133962631226\n",
      "Validation Accuracy: 9.43%\n",
      "Validation Loss: 2.4129\n",
      "Epoch [31/50], Loss: 1.0058964317495174\n",
      "Validation Accuracy: 31.60%\n",
      "Validation Loss: 1.5497\n",
      "Epoch [41/50], Loss: 0.9773329442197626\n",
      "Validation Accuracy: 37.67%\n",
      "Validation Loss: 1.4211\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "patience = 5  # Improvement 없을 때 기다릴 에포크 수\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)  # Adjusted based on the model's input\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)  # Validation loss 계산\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Prediction and accuracy calculation\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_loss = val_loss / len(valid_loader)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Validation Accuracy: {accuracy * 100:.2f}%')\n",
    "        print(f'Validation Loss: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2816, 532)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.concat([train_df.iloc[-24:,2:], test_df.iloc[:,2:]], axis=0)\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시퀀스 길이 및 특징 개수 정의\n",
    "seq_len = 24\n",
    "feature_n = 532\n",
    "\n",
    "# 슬라이딩 윈도우로 데이터 준비\n",
    "def create_sequences(X, seq_len):\n",
    "    sequences = []\n",
    "    for i in range(len(X) - seq_len):\n",
    "        seq = X[i:i + seq_len]\n",
    "        sequences.append(seq)\n",
    "    return torch.stack(sequences)\n",
    "\n",
    "# test_X_tensor: (2792, 532)\n",
    "test_X_tensor = torch.tensor(test_data.values, dtype=torch.float32)\n",
    "\n",
    "# 2. seq_len을 24로 설정하고, 시계열 데이터에 맞게 변환\n",
    "seq_len = 24\n",
    "test_X_seq = create_sequences(test_X_tensor, seq_len)\n",
    "\n",
    "# 3. DataLoader 생성 (배치 사이즈는 자유롭게 설정 가능, 여기서는 128)\n",
    "test_dataset = TensorDataset(test_X_seq)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 예측 수행\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "with torch.no_grad():\n",
    "    for inputs in test_loader:\n",
    "        inputs = inputs[0].to('cuda')  # 입력 데이터만 필요\n",
    "        outputs = model(inputs, None)  # 모델의 예측 결과\n",
    "        _, predicted = torch.max(outputs, 1)  # 예측 클래스 선택\n",
    "        test_predictions.extend(predicted.cpu().numpy())  # 결과를 리스트에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output file 할당후 save \n",
    "submission_df = submission_df.assign(target = test_predictions)\n",
    "submission_df.to_csv(\"output/output_patchTST.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dlinear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, configs, individual=False):\n",
    "        super(Model, self).__init__()\n",
    "        self.seq_len = configs['seq_len']\n",
    "        self.pred_len = configs['seq_len']\n",
    "        # Series decomposition block from Autoformer\n",
    "        self.decompsition = series_decomp(configs['moving_avg'])\n",
    "        self.individual = individual\n",
    "        self.channels = configs['enc_in']\n",
    "\n",
    "        if self.individual:\n",
    "            self.Linear_Seasonal = nn.ModuleList()\n",
    "            self.Linear_Trend = nn.ModuleList()\n",
    "\n",
    "            for i in range(self.channels):\n",
    "                self.Linear_Seasonal.append(\n",
    "                    nn.Linear(self.seq_len, self.pred_len))\n",
    "                self.Linear_Trend.append(\n",
    "                    nn.Linear(self.seq_len, self.pred_len))\n",
    "\n",
    "                self.Linear_Seasonal[i].weight = nn.Parameter(\n",
    "                    (1 / self.seq_len) * torch.ones([self.pred_len, self.seq_len]))\n",
    "                self.Linear_Trend[i].weight = nn.Parameter(\n",
    "                    (1 / self.seq_len) * torch.ones([self.pred_len, self.seq_len]))\n",
    "        else:\n",
    "            self.Linear_Seasonal = nn.Linear(self.seq_len, self.pred_len)\n",
    "            self.Linear_Trend = nn.Linear(self.seq_len, self.pred_len)\n",
    "\n",
    "            self.Linear_Seasonal.weight = nn.Parameter(\n",
    "                (1 / self.seq_len) * torch.ones([self.pred_len, self.seq_len]))\n",
    "            self.Linear_Trend.weight = nn.Parameter(\n",
    "                (1 / self.seq_len) * torch.ones([self.pred_len, self.seq_len]))\n",
    "\n",
    "        self.projection = nn.Linear(\n",
    "            configs['enc_in'] * configs['seq_len'], configs['num_class'])\n",
    "\n",
    "    def encoder(self, x):\n",
    "        seasonal_init, trend_init = self.decompsition(x)\n",
    "        seasonal_init, trend_init = seasonal_init.permute(\n",
    "            0, 2, 1), trend_init.permute(0, 2, 1)\n",
    "        if self.individual:\n",
    "            seasonal_output = torch.zeros([seasonal_init.size(0), seasonal_init.size(1), self.pred_len],\n",
    "                                          dtype=seasonal_init.dtype).to(seasonal_init.device)\n",
    "            trend_output = torch.zeros([trend_init.size(0), trend_init.size(1), self.pred_len],\n",
    "                                       dtype=trend_init.dtype).to(trend_init.device)\n",
    "            for i in range(self.channels):\n",
    "                seasonal_output[:, i, :] = self.Linear_Seasonal[i](\n",
    "                    seasonal_init[:, i, :])\n",
    "                trend_output[:, i, :] = self.Linear_Trend[i](\n",
    "                    trend_init[:, i, :])\n",
    "        else:\n",
    "            seasonal_output = self.Linear_Seasonal(seasonal_init)\n",
    "            trend_output = self.Linear_Trend(trend_init)\n",
    "        x = seasonal_output + trend_output\n",
    "        return x.permute(0, 2, 1)\n",
    "\n",
    "    def classification(self, x_enc):\n",
    "        # Encoder\n",
    "        enc_out = self.encoder(x_enc)\n",
    "        # Output\n",
    "        # (batch_size, seq_length * d_model)\n",
    "        output = enc_out.reshape(enc_out.shape[0], -1)\n",
    "        # (batch_size, num_classes)\n",
    "        output = self.projection(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, x_enc, mask=None):\n",
    "        dec_out = self.classification(x_enc)\n",
    "        return dec_out  # [B, N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data split for training\n",
    "train_data = train_df.iloc[:,2:]\n",
    "valid_data = train_df[\"target\"]\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_data, valid_data, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "# 3. 데이터 텐서 변환\n",
    "train_X_tensor = torch.tensor(x_train.values, dtype=torch.float32)\n",
    "train_y_tensor = torch.tensor(y_train.values.squeeze(), dtype=torch.long)\n",
    "valid_X_tensor = torch.tensor(x_valid.values, dtype=torch.float32)\n",
    "valid_y_tensor = torch.tensor(y_valid.values.squeeze(), dtype=torch.long)\n",
    "\n",
    "# 4. 데이터셋과 DataLoader 준비\n",
    "# seq_len 설정\n",
    "seq_len = 24\n",
    "batch_size = 64\n",
    "\n",
    "# 5. 시계열 데이터에 맞게 3차원 텐서로 변환\n",
    "def create_sequences(X, y, seq_len):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(X) - seq_len):\n",
    "        seq = X[i:i + seq_len]\n",
    "        label = y[i + seq_len]  # 다음 시간 스텝의 레이블\n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "    return torch.stack(sequences), torch.tensor(labels)\n",
    "\n",
    "train_X_seq, train_y_seq = create_sequences(train_X_tensor, train_y_tensor, seq_len)\n",
    "valid_X_seq, valid_y_seq = create_sequences(valid_X_tensor, valid_y_tensor, seq_len)\n",
    "\n",
    "# DataLoader 설정\n",
    "train_dataset = TensorDataset(train_X_seq, train_y_seq)\n",
    "valid_dataset = TensorDataset(valid_X_seq, valid_y_seq)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 모델 학습 및 검증\n",
    "configs = {\n",
    "    'seq_len': seq_len,\n",
    "    'moving_avg': 12, \n",
    "    'enc_in': x_train.shape[1],  # 입력 특성의 수\n",
    "    'num_class': 4  # 클래스 수\n",
    "}\n",
    "\n",
    "# 모델 생성\n",
    "model = Model(configs=configs, individual=False).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (24) must match the size of tensor b (23) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjusted based on the model's input\u001b[39;00m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[44], line 69\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x_enc, mask)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_enc, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 69\u001b[0m     dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dec_out\n",
      "Cell \u001b[0;32mIn[44], line 60\u001b[0m, in \u001b[0;36mModel.classification\u001b[0;34m(self, x_enc)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclassification\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_enc):\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# Encoder\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     enc_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# Output\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# (batch_size, seq_length * d_model)\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     output \u001b[38;5;241m=\u001b[39m enc_out\u001b[38;5;241m.\u001b[39mreshape(enc_out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[44], line 39\u001b[0m, in \u001b[0;36mModel.encoder\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencoder\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 39\u001b[0m     seasonal_init, trend_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecompsition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     seasonal_init, trend_init \u001b[38;5;241m=\u001b[39m seasonal_init\u001b[38;5;241m.\u001b[39mpermute(\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m), trend_init\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindividual:\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/level1_project/code/test/layers/autoformer_encdec.py:52\u001b[0m, in \u001b[0;36mseries_decomp.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     51\u001b[0m     moving_mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoving_avg(x)\n\u001b[0;32m---> 52\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmoving_mean\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res, moving_mean\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (24) must match the size of tensor b (23) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "patience = 5  # Improvement 없을 때 기다릴 에포크 수\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)  # Adjusted based on the model's input\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)  # Validation loss 계산\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Prediction and accuracy calculation\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_loss = val_loss / len(valid_loader)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Validation Accuracy: {accuracy * 100:.2f}%')\n",
    "        print(f'Validation Loss: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SegRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, configs):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # get parameters\n",
    "        self.seq_len = configs['seq_len']\n",
    "        self.enc_in = configs['enc_in']\n",
    "        self.d_model = configs['d_model']\n",
    "        self.dropout = configs['dropout']\n",
    "\n",
    "        self.pred_len = configs['seq_len']\n",
    "\n",
    "        self.seg_len = configs['seg_len']\n",
    "        self.seg_num_x = self.seq_len // self.seg_len\n",
    "        self.seg_num_y = self.pred_len // self.seg_len\n",
    "\n",
    "        # building model\n",
    "        self.valueEmbedding = nn.Sequential(\n",
    "            nn.Linear(self.seg_len, self.d_model),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.rnn = nn.GRU(input_size=self.d_model, hidden_size=self.d_model, num_layers=1, bias=True,\n",
    "                              batch_first=True, bidirectional=False)\n",
    "        self.pos_emb = nn.Parameter(torch.randn(self.seg_num_y, self.d_model // 2))\n",
    "        self.channel_emb = nn.Parameter(torch.randn(self.enc_in, self.d_model // 2))\n",
    "\n",
    "        self.predict = nn.Sequential(\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.d_model, self.seg_len)\n",
    "        )\n",
    "\n",
    "        self.act = F.gelu\n",
    "        self.dropout = nn.Dropout(configs['dropout'])\n",
    "        self.projection = nn.Linear(\n",
    "            configs['enc_in'] * configs['seq_len'], configs['num_class'])\n",
    "\n",
    "    def encoder(self, x):\n",
    "        # b:batch_size c:channel_size s:seq_len s:seq_len\n",
    "        # d:d_model w:seg_len n:seg_num_x m:seg_num_y\n",
    "        batch_size = x.size(0)\n",
    "        seq_last = x[:, -1:, :].detach()\n",
    "        x = (x - seq_last).permute(0, 2, 1) # b,c,s\n",
    "\n",
    "        x = self.valueEmbedding(x.reshape(-1, self.seg_num_x, self.seg_len))\n",
    "\n",
    "        # encoding\n",
    "        _, hn = self.rnn(x) # bc,n,d  1,bc,d\n",
    "\n",
    "        # m,d//2 -> 1,m,d//2 -> c,m,d//2\n",
    "        # c,d//2 -> c,1,d//2 -> c,m,d//2\n",
    "        # c,m,d -> cm,1,d -> bcm, 1, d\n",
    "        pos_emb = torch.cat([\n",
    "            self.pos_emb.unsqueeze(0).repeat(self.enc_in, 1, 1),\n",
    "            self.channel_emb.unsqueeze(1).repeat(1, self.seg_num_y, 1)\n",
    "        ], dim=-1).view(-1, 1, self.d_model).repeat(batch_size,1,1)\n",
    "\n",
    "        _, hy = self.rnn(pos_emb, hn.repeat(1, 1, self.seg_num_y).view(1, -1, self.d_model)) # bcm,1,d  1,bcm,d\n",
    "\n",
    "        # 1,bcm,d -> 1,bcm,w -> b,c,s\n",
    "        y = self.predict(hy).view(-1, self.enc_in, self.pred_len)\n",
    "\n",
    "        # permute and denorm\n",
    "        y = y.permute(0, 2, 1) + seq_last\n",
    "        return y\n",
    "\n",
    "    def classification(self, x_enc):\n",
    "        # Encoder\n",
    "        enc_out = self.encoder(x_enc)\n",
    "        # Output\n",
    "        # (batch_size, seq_length * d_model)\n",
    "        output = enc_out.reshape(enc_out.shape[0], -1)\n",
    "        # (batch_size, num_classes)\n",
    "        output = self.projection(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, x_enc):\n",
    "        dec_out = self.classification(x_enc)\n",
    "        return dec_out  # [B, N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data split for training\n",
    "train_data = train_df.iloc[:,2:]\n",
    "valid_data = train_df[\"target\"]\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_data, valid_data, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "# 3. 데이터 텐서 변환\n",
    "train_X_tensor = torch.tensor(x_train.values, dtype=torch.float32)\n",
    "train_y_tensor = torch.tensor(y_train.values.squeeze(), dtype=torch.long)\n",
    "valid_X_tensor = torch.tensor(x_valid.values, dtype=torch.float32)\n",
    "valid_y_tensor = torch.tensor(y_valid.values.squeeze(), dtype=torch.long)\n",
    "\n",
    "# 4. 데이터셋과 DataLoader 준비\n",
    "# seq_len 설정\n",
    "seq_len = 24\n",
    "batch_size = 128\n",
    "\n",
    "# 5. 시계열 데이터에 맞게 3차원 텐서로 변환\n",
    "def create_sequences(X, y, seq_len):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(X) - seq_len):\n",
    "        seq = X[i:i + seq_len]\n",
    "        label = y[i + seq_len]  # 다음 시간 스텝의 레이블\n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "    return torch.stack(sequences), torch.tensor(labels)\n",
    "\n",
    "train_X_seq, train_y_seq = create_sequences(train_X_tensor, train_y_tensor, seq_len)\n",
    "valid_X_seq, valid_y_seq = create_sequences(valid_X_tensor, valid_y_tensor, seq_len)\n",
    "\n",
    "# DataLoader 설정\n",
    "train_dataset = TensorDataset(train_X_seq, train_y_seq)\n",
    "valid_dataset = TensorDataset(valid_X_seq, valid_y_seq)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 24, 532])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "configs = {\n",
    "    'seq_len': 24,    # For example, define your sequence length\n",
    "    'enc_in': 532,    # Input feature dimension\n",
    "    'd_model': 128,    # Model dimension\n",
    "    'dropout': 0.1,   # Dropout rate\n",
    "    'seg_len': 6,     # Define segment length\n",
    "    'num_class': 4    # Number of classes for classification\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = Model(configs)\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Test Accuracy: 40.05%\n",
      "Epoch [2/10], Test Accuracy: 38.54%\n",
      "Epoch [3/10], Test Accuracy: 26.04%\n",
      "Epoch [4/10], Test Accuracy: 28.59%\n",
      "Epoch [5/10], Test Accuracy: 40.51%\n",
      "Epoch [6/10], Test Accuracy: 11.34%\n",
      "Epoch [7/10], Test Accuracy: 21.30%\n",
      "Epoch [8/10], Test Accuracy: 11.81%\n",
      "Epoch [9/10], Test Accuracy: 11.11%\n",
      "Epoch [10/10], Test Accuracy: 13.14%\n"
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Calculate loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update parameters\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, valid_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Get the class with the highest score\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 10  # Number of epochs to train\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, train_loader, criterion, optimizer, device)\n",
    "    accuracy = evaluate(model, valid_loader, device)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Test Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timemixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFT_series_decomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, top_k=5):\n",
    "        super(DFT_series_decomp, self).__init__()\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def forward(self, x):\n",
    "        xf = torch.fft.rfft(x)\n",
    "        freq = abs(xf)\n",
    "        freq[0] = 0\n",
    "        top_k_freq, top_list = torch.topk(freq, 5)\n",
    "        xf[freq <= top_k_freq.min()] = 0\n",
    "        x_season = torch.fft.irfft(xf)\n",
    "        x_trend = x - x_season\n",
    "        return x_season, x_trend\n",
    "\n",
    "\n",
    "class MultiScaleSeasonMixing(nn.Module):\n",
    "    \"\"\"\n",
    "    Bottom-up mixing season pattern\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, configs):\n",
    "        super(MultiScaleSeasonMixing, self).__init__()\n",
    "\n",
    "        self.down_sampling_layers = torch.nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    torch.nn.Linear(\n",
    "                        configs['seq_len'] // (configs['down_sampling_window'] ** i),\n",
    "                        configs['seq_len'] // (configs['down_sampling_window'] ** (i + 1)),\n",
    "                    ),\n",
    "                    nn.GELU(),\n",
    "                    torch.nn.Linear(\n",
    "                        configs['seq_len'] // (configs['down_sampling_window'] ** (i + 1)),\n",
    "                        configs['seq_len'] // (configs['down_sampling_window'] ** (i + 1)),\n",
    "                    ),\n",
    "\n",
    "                )\n",
    "                for i in range(configs['down_sampling_layers'])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, season_list):\n",
    "\n",
    "        # mixing high->low\n",
    "        out_high = season_list[0]\n",
    "        out_low = season_list[1]\n",
    "        out_season_list = [out_high.permute(0, 2, 1)]\n",
    "\n",
    "        for i in range(len(season_list) - 1):\n",
    "            out_low_res = self.down_sampling_layers[i](out_high)\n",
    "            out_low = out_low + out_low_res\n",
    "            out_high = out_low\n",
    "            if i + 2 <= len(season_list) - 1:\n",
    "                out_low = season_list[i + 2]\n",
    "            out_season_list.append(out_high.permute(0, 2, 1))\n",
    "\n",
    "        return out_season_list\n",
    "\n",
    "\n",
    "class MultiScaleTrendMixing(nn.Module):\n",
    "    \"\"\"\n",
    "    Top-down mixing trend pattern\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, configs):\n",
    "        super(MultiScaleTrendMixing, self).__init__()\n",
    "\n",
    "        self.up_sampling_layers = torch.nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    torch.nn.Linear(\n",
    "                        configs['seq_len'] // (configs['down_sampling_window'] ** (i + 1)),\n",
    "                        configs['seq_len'] // (configs['down_sampling_window'] ** i),\n",
    "                    ),\n",
    "                    nn.GELU(),\n",
    "                    torch.nn.Linear(\n",
    "                        configs['seq_len'] // (configs['down_sampling_window'] ** i),\n",
    "                        configs['seq_len'] // (configs['down_sampling_window'] ** i),\n",
    "                    ),\n",
    "                )\n",
    "                for i in reversed(range(configs['down_sampling_layers']))\n",
    "            ])\n",
    "\n",
    "    def forward(self, trend_list):\n",
    "\n",
    "        # mixing low->high\n",
    "        trend_list_reverse = trend_list.copy()\n",
    "        trend_list_reverse.reverse()\n",
    "        out_low = trend_list_reverse[0]\n",
    "        out_high = trend_list_reverse[1]\n",
    "        out_trend_list = [out_low.permute(0, 2, 1)]\n",
    "\n",
    "        for i in range(len(trend_list_reverse) - 1):\n",
    "            out_high_res = self.up_sampling_layers[i](out_low)\n",
    "            out_high = out_high + out_high_res\n",
    "            out_low = out_high\n",
    "            if i + 2 <= len(trend_list_reverse) - 1:\n",
    "                out_high = trend_list_reverse[i + 2]\n",
    "            out_trend_list.append(out_low.permute(0, 2, 1))\n",
    "\n",
    "        out_trend_list.reverse()\n",
    "        return out_trend_list\n",
    "\n",
    "\n",
    "class PastDecomposableMixing(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        super(PastDecomposableMixing, self).__init__()\n",
    "        self.seq_len = configs['seq_len']\n",
    "        self.pred_len = configs['pred_len']\n",
    "        self.down_sampling_window = configs['down_sampling_window']\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(configs['d_model'])\n",
    "        self.dropout = nn.Dropout(configs['dropout'])\n",
    "        self.channel_independence = configs['channel_independence']\n",
    "\n",
    "        if configs['decomp_method'] == 'moving_avg':\n",
    "            self.decompsition = series_decomp(configs['moving_avg'])\n",
    "        elif configs['decomp_method'] == \"dft_decomp\":\n",
    "            self.decompsition = DFT_series_decomp(configs['top_k'])\n",
    "        else:\n",
    "            raise ValueError('decompsition is error')\n",
    "\n",
    "        if not configs['channel_independence']:\n",
    "            self.cross_layer = nn.Sequential(\n",
    "                nn.Linear(in_features=configs['d_model'], out_features=configs['d_ff']),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(in_features=configs['d_ff'], out_features=configs['d_model']),\n",
    "            )\n",
    "\n",
    "        # Mixing season\n",
    "        self.mixing_multi_scale_season = MultiScaleSeasonMixing(configs)\n",
    "\n",
    "        # Mxing trend\n",
    "        self.mixing_multi_scale_trend = MultiScaleTrendMixing(configs)\n",
    "\n",
    "        self.out_cross_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=configs['d_model'], out_features=configs['d_ff']),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_features=configs['d_ff'], out_features=configs['d_model']),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_list):\n",
    "        length_list = []\n",
    "        for x in x_list:\n",
    "            _, T, _ = x.size()\n",
    "            length_list.append(T)\n",
    "\n",
    "        # Decompose to obtain the season and trend\n",
    "        season_list = []\n",
    "        trend_list = []\n",
    "        for x in x_list:\n",
    "            season, trend = self.decompsition(x)\n",
    "            if not self.channel_independence:\n",
    "                season = self.cross_layer(season)\n",
    "                trend = self.cross_layer(trend)\n",
    "            season_list.append(season.permute(0, 2, 1))\n",
    "            trend_list.append(trend.permute(0, 2, 1))\n",
    "\n",
    "        # bottom-up season mixing\n",
    "        out_season_list = self.mixing_multi_scale_season(season_list)\n",
    "        # top-down trend mixing\n",
    "        out_trend_list = self.mixing_multi_scale_trend(trend_list)\n",
    "\n",
    "        out_list = []\n",
    "        for ori, out_season, out_trend, length in zip(x_list, out_season_list, out_trend_list,\n",
    "                                                      length_list):\n",
    "            out = out_season + out_trend\n",
    "            if self.channel_independence:\n",
    "                out = ori + self.out_cross_layer(out)\n",
    "            out_list.append(out[:, :length, :])\n",
    "        return out_list\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, configs):\n",
    "        super(Model, self).__init__()\n",
    "        self.configs = configs\n",
    "        self.seq_len = configs['seq_len']\n",
    "        self.label_len = configs['label_len']\n",
    "        self.pred_len = configs['pred_len']\n",
    "        self.down_sampling_window = configs['down_sampling_window']\n",
    "        self.channel_independence = configs['channel_independence']\n",
    "        self.pdm_blocks = nn.ModuleList([PastDecomposableMixing(configs)\n",
    "                                         for _ in range(configs['e_layers'])])\n",
    "\n",
    "        self.preprocess = series_decomp(configs['moving_avg'])\n",
    "        self.enc_in = configs['enc_in']\n",
    "\n",
    "        if self.channel_independence:\n",
    "            self.enc_embedding = DataEmbedding_wo_pos(1, configs['d_model'], configs['embed'], configs['freq'],\n",
    "                                                      configs['dropout'])\n",
    "        else:\n",
    "            self.enc_embedding = DataEmbedding_wo_pos(configs['enc_in'], configs['d_model'], configs['embed'], configs['freq'],\n",
    "                                                      configs['dropout'])\n",
    "\n",
    "        self.layer = configs['e_layers']\n",
    "\n",
    "        self.normalize_layers = torch.nn.ModuleList(\n",
    "            [\n",
    "                Normalize(self.configs['enc_in'], affine=True, non_norm=True if configs['use_norm'] == 0 else False)\n",
    "                for i in range(configs['down_sampling_layers'] + 1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.act = F.gelu\n",
    "        self.dropout = nn.Dropout(configs['dropout'])\n",
    "        self.projection = nn.Linear(\n",
    "            configs['d_model'] * configs['seq_len'], configs['num_class'])\n",
    "\n",
    "    def out_projection(self, dec_out, i, out_res):\n",
    "        dec_out = self.projection_layer(dec_out)\n",
    "        out_res = out_res.permute(0, 2, 1)\n",
    "        out_res = self.out_res_layers[i](out_res)\n",
    "        out_res = self.regression_layers[i](out_res).permute(0, 2, 1)\n",
    "        dec_out = dec_out + out_res\n",
    "        return dec_out\n",
    "\n",
    "    def pre_enc(self, x_list):\n",
    "        if self.channel_independence:\n",
    "            return (x_list, None)\n",
    "        else:\n",
    "            out1_list = []\n",
    "            out2_list = []\n",
    "            for x in x_list:\n",
    "                x_1, x_2 = self.preprocess(x)\n",
    "                out1_list.append(x_1)\n",
    "                out2_list.append(x_2)\n",
    "            return (out1_list, out2_list)\n",
    "\n",
    "    def __multi_scale_process_inputs(self, x_enc, x_mark_enc):\n",
    "        if self.configs['down_sampling_method'] == 'max':\n",
    "            down_pool = torch.nn.MaxPool1d(self.configs['down_sampling_window'], return_indices=False)\n",
    "        elif self.configs['down_sampling_method'] == 'avg':\n",
    "            down_pool = torch.nn.AvgPool1d(self.configs['down_sampling_window'])\n",
    "        elif self.configs['down_sampling_method'] == 'conv':\n",
    "            padding = 1 if torch.__version__ >= '1.5.0' else 2\n",
    "            down_pool = nn.Conv1d(in_channels=self.configs['enc_in'], out_channels=self.configs['enc_in'],\n",
    "                                  kernel_size=3, padding=padding,\n",
    "                                  stride=self.configs['down_sampling_window'],\n",
    "                                  padding_mode='circular',\n",
    "                                  bias=False)\n",
    "        else:\n",
    "            return x_enc, x_mark_enc\n",
    "        # B,T,C -> B,C,T\n",
    "        x_enc = x_enc.permute(0, 2, 1)\n",
    "\n",
    "        x_enc_ori = x_enc\n",
    "        x_mark_enc_mark_ori = x_mark_enc\n",
    "\n",
    "        x_enc_sampling_list = []\n",
    "        x_mark_sampling_list = []\n",
    "        x_enc_sampling_list.append(x_enc.permute(0, 2, 1))\n",
    "        x_mark_sampling_list.append(x_mark_enc)\n",
    "\n",
    "        for i in range(self.configs['down_sampling_layers']):\n",
    "            x_enc_sampling = down_pool(x_enc_ori)\n",
    "\n",
    "            x_enc_sampling_list.append(x_enc_sampling.permute(0, 2, 1))\n",
    "            x_enc_ori = x_enc_sampling\n",
    "\n",
    "            if x_mark_enc is not None:\n",
    "                x_mark_sampling_list.append(x_mark_enc_mark_ori[:, ::self.configs['down_sampling_window'], :])\n",
    "                x_mark_enc_mark_ori = x_mark_enc_mark_ori[:, ::self.configs['down_sampling_window'], :]\n",
    "\n",
    "        x_enc = x_enc_sampling_list\n",
    "        x_mark_enc = x_mark_sampling_list if x_mark_enc is not None else None\n",
    "\n",
    "        return x_enc, x_mark_enc\n",
    "\n",
    "\n",
    "    def future_multi_mixing(self, B, enc_out_list, x_list):\n",
    "        dec_out_list = []\n",
    "        if self.channel_independence:\n",
    "            x_list = x_list[0]\n",
    "            for i, enc_out in zip(range(len(x_list)), enc_out_list):\n",
    "                dec_out = self.predict_layers[i](enc_out.permute(0, 2, 1)).permute(\n",
    "                    0, 2, 1)  # align temporal dimension\n",
    "                dec_out = self.projection_layer(dec_out)\n",
    "                dec_out = dec_out.reshape(B, self.configs.c_out, self.pred_len).permute(0, 2, 1).contiguous()\n",
    "                dec_out_list.append(dec_out)\n",
    "\n",
    "        else:\n",
    "            for i, enc_out, out_res in zip(range(len(x_list[0])), enc_out_list, x_list[1]):\n",
    "                dec_out = self.predict_layers[i](enc_out.permute(0, 2, 1)).permute(\n",
    "                    0, 2, 1)  # align temporal dimension\n",
    "                dec_out = self.out_projection(dec_out, i, out_res)\n",
    "                dec_out_list.append(dec_out)\n",
    "\n",
    "        return dec_out_list\n",
    "\n",
    "    def classification(self, x_enc):\n",
    "        x_enc, _ = self.__multi_scale_process_inputs(x_enc, None)\n",
    "        x_list = x_enc\n",
    "\n",
    "        # embedding\n",
    "        enc_out_list = []\n",
    "        for x in x_list:\n",
    "            enc_out = self.enc_embedding(x, None)  # [B,T,C]\n",
    "            enc_out_list.append(enc_out)\n",
    "\n",
    "        # MultiScale-CrissCrossAttention  as encoder for past\n",
    "        for i in range(self.layer):\n",
    "            enc_out_list = self.pdm_blocks[i](enc_out_list)\n",
    "\n",
    "        enc_out = enc_out_list[0]\n",
    "        # Output\n",
    "        # the output transformer encoder/decoder embeddings don't include non-linearity\n",
    "        output = self.act(enc_out)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        # (batch_size, seq_length * d_model)\n",
    "        output = output.reshape(output.shape[0], -1)\n",
    "        output = self.projection(output)  # (batch_size, num_classes)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def forward(self, x_enc):\n",
    "        dec_out = self.classification(x_enc)\n",
    "        return dec_out  # [B, N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "        'seq_len': 24,  # 입력 시퀀스 길이\n",
    "        'label_len': 1,  # 라벨 길이\n",
    "        'pred_len': 1,  # 예측할 길이\n",
    "        'down_sampling_window': 24,  # 다운샘플링 윈도우\n",
    "        'channel_independence': True,  # 채널 독립성 여부\n",
    "        'e_layers': 2,  # 인코더 레이어 수\n",
    "        'moving_avg': 3,  # 이동 평균\n",
    "        'enc_in': 532,  # 입력 차원\n",
    "        'd_model': 64,  # 모델의 차원\n",
    "        'd_ff' : 32,\n",
    "        'embed': 'fixed',  # 임베딩 방법\n",
    "        'decomp_method' : 'moving_avg',\n",
    "        'freq': 'h',  # 데이터의 주기 (예: 시간 단위)\n",
    "        'dropout': 0.1,  # 드롭아웃 비율\n",
    "        'num_class': 4,  # 출력 클래스 수\n",
    "        'use_norm': 1,  # 정규화 사용 여부\n",
    "        'down_sampling_layers': 2,  # 다운샘플링 레이어 수\n",
    "        'down_sampling_method': 'avg',  # 다운샘플링 방법 ('max', 'avg', 'conv' 중 선택)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data split for training\n",
    "train_data = train_df.iloc[:,2:]\n",
    "valid_data = train_df[\"target\"]\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_data, valid_data, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "# 3. 데이터 텐서 변환\n",
    "train_X_tensor = torch.tensor(x_train.values, dtype=torch.float32)\n",
    "train_y_tensor = torch.tensor(y_train.values.squeeze(), dtype=torch.long)\n",
    "valid_X_tensor = torch.tensor(x_valid.values, dtype=torch.float32)\n",
    "valid_y_tensor = torch.tensor(y_valid.values.squeeze(), dtype=torch.long)\n",
    "\n",
    "# 4. 데이터셋과 DataLoader 준비\n",
    "# seq_len 설정\n",
    "seq_len = 24\n",
    "batch_size = 64\n",
    "\n",
    "# 5. 시계열 데이터에 맞게 3차원 텐서로 변환\n",
    "def create_sequences(X, y, seq_len):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(X) - seq_len):\n",
    "        seq = X[i:i + seq_len]\n",
    "        label = y[i + seq_len]  # 다음 시간 스텝의 레이블\n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "    return torch.stack(sequences), torch.tensor(labels)\n",
    "\n",
    "train_X_seq, train_y_seq = create_sequences(train_X_tensor, train_y_tensor, seq_len)\n",
    "valid_X_seq, valid_y_seq = create_sequences(valid_X_tensor, valid_y_tensor, seq_len)\n",
    "\n",
    "# DataLoader 설정\n",
    "train_dataset = TensorDataset(train_X_seq, train_y_seq)\n",
    "valid_dataset = TensorDataset(valid_X_seq, valid_y_seq)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = Model(configs)\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (532x1x1). Calculated output size: (532x1x0). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[71], line 324\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x_enc)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_enc):\n\u001b[0;32m--> 324\u001b[0m     dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dec_out\n",
      "Cell \u001b[0;32mIn[71], line 298\u001b[0m, in \u001b[0;36mModel.classification\u001b[0;34m(self, x_enc)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclassification\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_enc):\n\u001b[0;32m--> 298\u001b[0m     x_enc, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__multi_scale_process_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     x_list \u001b[38;5;241m=\u001b[39m x_enc\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;66;03m# embedding\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[71], line 262\u001b[0m, in \u001b[0;36mModel.__multi_scale_process_inputs\u001b[0;34m(self, x_enc, x_mark_enc)\u001b[0m\n\u001b[1;32m    259\u001b[0m x_mark_sampling_list\u001b[38;5;241m.\u001b[39mappend(x_mark_enc)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfigs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdown_sampling_layers\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m--> 262\u001b[0m     x_enc_sampling \u001b[38;5;241m=\u001b[39m \u001b[43mdown_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc_ori\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m     x_enc_sampling_list\u001b[38;5;241m.\u001b[39mappend(x_enc_sampling\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    265\u001b[0m     x_enc_ori \u001b[38;5;241m=\u001b[39m x_enc_sampling\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/pooling.py:555\u001b[0m, in \u001b[0;36mAvgPool1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavg_pool1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount_include_pad\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given input size: (532x1x1). Calculated output size: (532x1x0). Output size is too small"
     ]
    }
   ],
   "source": [
    "model(next(iter(train_loader))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (532x1x1). Calculated output size: (532x1x0). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# Number of epochs to train\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 36\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m evaluate(model, valid_loader, device)\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[69], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m      5\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Zero the gradients\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)  \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[1;32m     10\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[51], line 325\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x_enc)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_enc):\n\u001b[0;32m--> 325\u001b[0m     dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dec_out\n",
      "Cell \u001b[0;32mIn[51], line 298\u001b[0m, in \u001b[0;36mModel.classification\u001b[0;34m(self, x_enc)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclassification\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_enc):\n\u001b[0;32m--> 298\u001b[0m     x_enc, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__multi_scale_process_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     x_list \u001b[38;5;241m=\u001b[39m x_enc\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;66;03m# embedding\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[51], line 262\u001b[0m, in \u001b[0;36mModel.__multi_scale_process_inputs\u001b[0;34m(self, x_enc, x_mark_enc)\u001b[0m\n\u001b[1;32m    259\u001b[0m x_mark_sampling_list\u001b[38;5;241m.\u001b[39mappend(x_mark_enc)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfigs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdown_sampling_layers\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m--> 262\u001b[0m     x_enc_sampling \u001b[38;5;241m=\u001b[39m \u001b[43mdown_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc_ori\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m     x_enc_sampling_list\u001b[38;5;241m.\u001b[39mappend(x_enc_sampling\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    265\u001b[0m     x_enc_ori \u001b[38;5;241m=\u001b[39m x_enc_sampling\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/pooling.py:555\u001b[0m, in \u001b[0;36mAvgPool1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavg_pool1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount_include_pad\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given input size: (532x1x1). Calculated output size: (532x1x0). Output size is too small"
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Calculate loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update parameters\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, valid_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Get the class with the highest score\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 10  # Number of epochs to train\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, train_loader, criterion, optimizer, device)\n",
    "    accuracy = evaluate(model, valid_loader, device)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Test Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
