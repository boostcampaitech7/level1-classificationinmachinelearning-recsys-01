{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu121\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import add_dummy_feature\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# statsmodel\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from pmdarima.arima import auto_arima\n",
    "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf \n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.arima_model import ARIMAResults\n",
    "from statsmodels.multivariate.manova import MANOVA\n",
    "\n",
    "\n",
    "# model\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch.optim as optim\n",
    "from layers.transformer_encdec import Encoder, EncoderLayer\n",
    "from layers.selfattention_family import FullAttention, AttentionLayer\n",
    "from layers.embed import PatchEmbedding, DataEmbedding_wo_pos\n",
    "from layers.autoformer_encdec import series_decomp, series_decomp_multi\n",
    "from layers.autoformer_encdec import series_decomp_fixed, series_decomp_fixed_multi\n",
    "from layers.standard import Normalize\n",
    "\n",
    "print(torch.__version__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 호출\n",
    "data_path: str = \"../../../data\"\n",
    "train_df: pd.DataFrame = pd.read_csv(os.path.join(data_path, \"train.csv\")).assign(_type=\"train\") # train 에는 _type = train \n",
    "test_df: pd.DataFrame = pd.read_csv(os.path.join(data_path, \"test.csv\")).assign(_type=\"test\") # test 에는 _type = test\n",
    "submission_df: pd.DataFrame = pd.read_csv(os.path.join(data_path, \"test.csv\")) # ID, target 열만 가진 데이터 미리 호출\n",
    "df: pd.DataFrame = pd.concat([train_df, test_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:03<00:00, 30.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# HOURLY_ 로 시작하는 .csv 파일 이름을 file_names 에 할딩\n",
    "file_names: List[str] = [\n",
    "    f for f in os.listdir(data_path) if f.startswith(\"HOURLY_\") and f.endswith(\".csv\")\n",
    "]\n",
    "\n",
    "# 파일명 : 데이터프레임으로 딕셔너리 형태로 저장\n",
    "file_dict: Dict[str, pd.DataFrame] = {\n",
    "    f.replace(\".csv\", \"\"): pd.read_csv(os.path.join(data_path, f)) for f in file_names\n",
    "}\n",
    "\n",
    "for _file_name, _df in tqdm(file_dict.items()):\n",
    "    # 열 이름 중복 방지를 위해 {_file_name.lower()}_{col.lower()}로 변경, datetime 열을 ID로 변경\n",
    "    _rename_rule = {\n",
    "        col: f\"{_file_name.lower()}_{col.lower()}\" if col != \"datetime\" else \"ID\"\n",
    "        for col in _df.columns\n",
    "    }\n",
    "    _df = _df.rename(_rename_rule, axis=1)\n",
    "    df = df.merge(_df, on=\"ID\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing value check\n",
    "\n",
    "train_df = df.loc[df[\"_type\"] == \"train\"]\n",
    "# 각 열에서 누락된 값의 수 & 백분율 계산\n",
    "missing_values = train_df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(train_df)) * 100\n",
    "\n",
    "# 누락된 값 비율을 기준으로 열 정렬\n",
    "sorted_missing_percentage = missing_percentage.sort_values(ascending=False)\n",
    "\n",
    "# missing_value의 비율이 100%가 아닌 column만 추출\n",
    "non_missing_columns = sorted_missing_percentage[sorted_missing_percentage != 100.0].index.tolist()\n",
    "non_missing_columns.remove('ID')\n",
    "non_missing_columns.remove('target')\n",
    "non_missing_columns.remove('_type')\n",
    "\n",
    "new_data = train_df[['ID','target', '_type'] + non_missing_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이동평균으로 결측치 대체\n",
    "new_df_stab = new_data[non_missing_columns]\n",
    "\n",
    "# train\n",
    "window_size = 3\n",
    "new_df_stab = new_df_stab.apply(lambda col: col.fillna(col.rolling(window=window_size, min_periods=1).mean()))\n",
    "new_df_stab = new_df_stab.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "\n",
    "# 결측치 처리한 new_df 정의\n",
    "new_train_df = pd.concat([new_data[['ID','target','_type']], new_df_stab], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df.loc[df[\"_type\"] == \"test\"]\n",
    "new_test_df = test_df[['ID','target','_type'] + non_missing_columns]\n",
    "\n",
    "new_test_stab = new_test_df[non_missing_columns]\n",
    "# test\n",
    "window_size = 3\n",
    "new_test_stab = new_test_stab.apply(lambda col: col.fillna(col.rolling(window=window_size, min_periods=1).mean()))\n",
    "new_test_stab = new_test_stab.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "new_test_df = pd.concat([new_test_df[['ID','target','_type']], new_test_stab], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 비율을 계산\n",
    "missing_percentage = new_test_df.isnull().mean() * 100\n",
    "\n",
    "# 결측치 비율이 100%인 컬럼 이름만 출력\n",
    "columns_with_all_missing = missing_percentage[missing_percentage >= 50].index.tolist()\n",
    "\n",
    "# 100% 결측치가 있는 컬럼 출력\n",
    "columns_with_all_missing = [col for col in columns_with_all_missing if col not in ['target', 'hourly_market-data_price-ohlcv_all_exchange_spot_btc_usd_close']]\n",
    "\n",
    "# train_df와 test_df에서 columns_with_all_missing에 있는 컬럼 삭제\n",
    "new_train_df = new_train_df.drop(columns=columns_with_all_missing, errors='ignore')\n",
    "new_test_df = new_test_df.drop(columns=columns_with_all_missing, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이동평균을 기반으로 이상치를 처리하는 함수\n",
    "def replace_outlier(df, window=3, threshold=2):\n",
    "    df_cleaned = df.copy()\n",
    "    \n",
    "    # 숫자형 컬럼들에 대해 처리\n",
    "    for column in df_cleaned.select_dtypes(include=[np.number]).columns:\n",
    "        # 이동평균과 표준편차 계산\n",
    "        rolling_mean = df_cleaned[column].rolling(window=window, min_periods=1).mean()\n",
    "        rolling_std = df_cleaned[column].rolling(window=window, min_periods=1).std()\n",
    "\n",
    "        # 이상치 기준 설정\n",
    "        outliers = np.abs(df_cleaned[column] - rolling_mean) > (threshold * rolling_std)\n",
    "\n",
    "        # 이상치를 이동평균으로 대체\n",
    "        df_cleaned.loc[outliers, column] = rolling_mean[outliers]\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "# 이동평균 기반 이상치 처리 적용\n",
    "cleaned_train_df = replace_outlier(new_train_df)\n",
    "cleaned_test_df = replace_outlier(new_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardization(train_df, test_df):\n",
    "    features_to_scale = [col for col in train_df.columns if col not in ['ID', 'target', '_type']]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # 훈련 데이터 정규화\n",
    "    train_df_scaled = train_df.copy()\n",
    "    train_df_scaled[features_to_scale] = scaler.fit_transform(train_df[features_to_scale])\n",
    "\n",
    "    # 테스트 데이터 정규화\n",
    "    test_df_scaled = test_df.copy()\n",
    "    test_df_scaled[features_to_scale] = scaler.transform(test_df[features_to_scale])\n",
    "\n",
    "    return train_df_scaled, test_df_scaled\n",
    "\n",
    "# 함수 호출\n",
    "std_train_df, std_test_df = standardization(cleaned_train_df, cleaned_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Validation Set: 0.0006475219728316957\n"
     ]
    }
   ],
   "source": [
    "# 타겟과 피처 설정\n",
    "y_train = std_train_df['hourly_market-data_price-ohlcv_all_exchange_spot_btc_usd_close']\n",
    "X_train = std_train_df.drop(columns=['hourly_market-data_price-ohlcv_all_exchange_spot_btc_usd_close', 'ID', 'target', '_type'], errors='ignore')\n",
    "\n",
    "# 훈련 데이터와 검증 데이터 나누기\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# 모델 훈련\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# 검증 데이터에서 예측\n",
    "y_val_pred = model.predict(X_val)\n",
    "\n",
    "# 성능 평가\n",
    "mse = mean_squared_error(y_val, y_val_pred)\n",
    "print(\"Mean Squared Error on Validation Set:\", mse)\n",
    "\n",
    "# 모델 훈련\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# test_df에서 예측\n",
    "X_test = std_test_df.drop(columns=['hourly_market-data_price-ohlcv_all_exchange_spot_btc_usd_close', 'ID', 'target', '_type'], errors='ignore')\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# new_test_df에 y_pred 값을 추가\n",
    "std_test_df['hourly_market-data_price-ohlcv_all_exchange_spot_btc_usd_close'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "      <th>_type</th>\n",
       "      <th>hourly_market-data_funding-rates_bybit_funding_rates</th>\n",
       "      <th>hourly_market-data_taker-buy-sell-stats_bybit_taker_buy_volume</th>\n",
       "      <th>hourly_market-data_taker-buy-sell-stats_bybit_taker_sell_volume</th>\n",
       "      <th>hourly_market-data_taker-buy-sell-stats_bybit_taker_buy_ratio</th>\n",
       "      <th>hourly_market-data_taker-buy-sell-stats_bybit_taker_sell_ratio</th>\n",
       "      <th>hourly_market-data_taker-buy-sell-stats_bybit_taker_buy_sell_ratio</th>\n",
       "      <th>hourly_network-data_fees-transaction_fees_transaction_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>hourly_market-data_liquidations_binance_all_symbol_long_liquidations_usd</th>\n",
       "      <th>hourly_market-data_liquidations_binance_all_symbol_short_liquidations_usd</th>\n",
       "      <th>hourly_market-data_open-interest_htx_global_btc_usd_open_interest</th>\n",
       "      <th>hourly_network-data_addresses-count_addresses_count_receiver</th>\n",
       "      <th>hourly_network-data_fees_fees_total</th>\n",
       "      <th>hourly_network-data_fees_fees_total_usd</th>\n",
       "      <th>hourly_market-data_liquidations_htx_global_all_symbol_long_liquidations</th>\n",
       "      <th>hourly_market-data_liquidations_htx_global_all_symbol_short_liquidations</th>\n",
       "      <th>hourly_market-data_liquidations_htx_global_all_symbol_long_liquidations_usd</th>\n",
       "      <th>hourly_market-data_liquidations_htx_global_all_symbol_short_liquidations_usd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01 00:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>train</td>\n",
       "      <td>0.242277</td>\n",
       "      <td>-0.739005</td>\n",
       "      <td>-0.790786</td>\n",
       "      <td>0.677313</td>\n",
       "      <td>-0.660030</td>\n",
       "      <td>0.248331</td>\n",
       "      <td>-0.452496</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.327741</td>\n",
       "      <td>-0.346728</td>\n",
       "      <td>-0.099874</td>\n",
       "      <td>0.946969</td>\n",
       "      <td>-0.472265</td>\n",
       "      <td>-0.467622</td>\n",
       "      <td>-0.289134</td>\n",
       "      <td>-0.23536</td>\n",
       "      <td>-0.2937</td>\n",
       "      <td>-0.232519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-01 01:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>train</td>\n",
       "      <td>0.242277</td>\n",
       "      <td>-0.555248</td>\n",
       "      <td>-0.812900</td>\n",
       "      <td>2.230195</td>\n",
       "      <td>-2.214769</td>\n",
       "      <td>2.351257</td>\n",
       "      <td>-0.476448</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.328215</td>\n",
       "      <td>-0.319897</td>\n",
       "      <td>0.005954</td>\n",
       "      <td>-0.773384</td>\n",
       "      <td>-0.540988</td>\n",
       "      <td>-0.496686</td>\n",
       "      <td>-0.289134</td>\n",
       "      <td>-0.23536</td>\n",
       "      <td>-0.2937</td>\n",
       "      <td>-0.232519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-01 02:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>train</td>\n",
       "      <td>0.242277</td>\n",
       "      <td>-0.844533</td>\n",
       "      <td>-0.783052</td>\n",
       "      <td>-1.185413</td>\n",
       "      <td>1.204924</td>\n",
       "      <td>-0.646820</td>\n",
       "      <td>-0.417162</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.328215</td>\n",
       "      <td>-0.346728</td>\n",
       "      <td>-0.005201</td>\n",
       "      <td>-0.889786</td>\n",
       "      <td>-0.528197</td>\n",
       "      <td>-0.491262</td>\n",
       "      <td>-0.289134</td>\n",
       "      <td>-0.23536</td>\n",
       "      <td>-0.2937</td>\n",
       "      <td>-0.232519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-01 03:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>train</td>\n",
       "      <td>0.242277</td>\n",
       "      <td>-0.809874</td>\n",
       "      <td>-0.841254</td>\n",
       "      <td>0.671624</td>\n",
       "      <td>-0.654333</td>\n",
       "      <td>0.244119</td>\n",
       "      <td>-0.486115</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.304814</td>\n",
       "      <td>-0.346728</td>\n",
       "      <td>-0.036679</td>\n",
       "      <td>-0.447158</td>\n",
       "      <td>-0.549200</td>\n",
       "      <td>-0.500178</td>\n",
       "      <td>-0.289134</td>\n",
       "      <td>-0.23536</td>\n",
       "      <td>-0.2937</td>\n",
       "      <td>-0.232519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-01 04:00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>train</td>\n",
       "      <td>0.242277</td>\n",
       "      <td>-0.701877</td>\n",
       "      <td>-0.660023</td>\n",
       "      <td>-0.369732</td>\n",
       "      <td>0.388268</td>\n",
       "      <td>-0.346476</td>\n",
       "      <td>-0.449115</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.313955</td>\n",
       "      <td>-0.346728</td>\n",
       "      <td>-0.036056</td>\n",
       "      <td>0.343477</td>\n",
       "      <td>-0.521007</td>\n",
       "      <td>-0.488245</td>\n",
       "      <td>-0.289134</td>\n",
       "      <td>-0.23536</td>\n",
       "      <td>-0.2937</td>\n",
       "      <td>-0.232519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 209 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ID  target  _type  \\\n",
       "0  2023-01-01 00:00:00     2.0  train   \n",
       "1  2023-01-01 01:00:00     1.0  train   \n",
       "2  2023-01-01 02:00:00     1.0  train   \n",
       "3  2023-01-01 03:00:00     1.0  train   \n",
       "4  2023-01-01 04:00:00     2.0  train   \n",
       "\n",
       "   hourly_market-data_funding-rates_bybit_funding_rates  \\\n",
       "0                                           0.242277      \n",
       "1                                           0.242277      \n",
       "2                                           0.242277      \n",
       "3                                           0.242277      \n",
       "4                                           0.242277      \n",
       "\n",
       "   hourly_market-data_taker-buy-sell-stats_bybit_taker_buy_volume  \\\n",
       "0                                          -0.739005                \n",
       "1                                          -0.555248                \n",
       "2                                          -0.844533                \n",
       "3                                          -0.809874                \n",
       "4                                          -0.701877                \n",
       "\n",
       "   hourly_market-data_taker-buy-sell-stats_bybit_taker_sell_volume  \\\n",
       "0                                          -0.790786                 \n",
       "1                                          -0.812900                 \n",
       "2                                          -0.783052                 \n",
       "3                                          -0.841254                 \n",
       "4                                          -0.660023                 \n",
       "\n",
       "   hourly_market-data_taker-buy-sell-stats_bybit_taker_buy_ratio  \\\n",
       "0                                           0.677313               \n",
       "1                                           2.230195               \n",
       "2                                          -1.185413               \n",
       "3                                           0.671624               \n",
       "4                                          -0.369732               \n",
       "\n",
       "   hourly_market-data_taker-buy-sell-stats_bybit_taker_sell_ratio  \\\n",
       "0                                          -0.660030                \n",
       "1                                          -2.214769                \n",
       "2                                           1.204924                \n",
       "3                                          -0.654333                \n",
       "4                                           0.388268                \n",
       "\n",
       "   hourly_market-data_taker-buy-sell-stats_bybit_taker_buy_sell_ratio  \\\n",
       "0                                           0.248331                    \n",
       "1                                           2.351257                    \n",
       "2                                          -0.646820                    \n",
       "3                                           0.244119                    \n",
       "4                                          -0.346476                    \n",
       "\n",
       "   hourly_network-data_fees-transaction_fees_transaction_mean  ...  \\\n",
       "0                                          -0.452496           ...   \n",
       "1                                          -0.476448           ...   \n",
       "2                                          -0.417162           ...   \n",
       "3                                          -0.486115           ...   \n",
       "4                                          -0.449115           ...   \n",
       "\n",
       "   hourly_market-data_liquidations_binance_all_symbol_long_liquidations_usd  \\\n",
       "0                                          -0.327741                          \n",
       "1                                          -0.328215                          \n",
       "2                                          -0.328215                          \n",
       "3                                          -0.304814                          \n",
       "4                                          -0.313955                          \n",
       "\n",
       "   hourly_market-data_liquidations_binance_all_symbol_short_liquidations_usd  \\\n",
       "0                                          -0.346728                           \n",
       "1                                          -0.319897                           \n",
       "2                                          -0.346728                           \n",
       "3                                          -0.346728                           \n",
       "4                                          -0.346728                           \n",
       "\n",
       "   hourly_market-data_open-interest_htx_global_btc_usd_open_interest  \\\n",
       "0                                          -0.099874                   \n",
       "1                                           0.005954                   \n",
       "2                                          -0.005201                   \n",
       "3                                          -0.036679                   \n",
       "4                                          -0.036056                   \n",
       "\n",
       "   hourly_network-data_addresses-count_addresses_count_receiver  \\\n",
       "0                                           0.946969              \n",
       "1                                          -0.773384              \n",
       "2                                          -0.889786              \n",
       "3                                          -0.447158              \n",
       "4                                           0.343477              \n",
       "\n",
       "   hourly_network-data_fees_fees_total  \\\n",
       "0                            -0.472265   \n",
       "1                            -0.540988   \n",
       "2                            -0.528197   \n",
       "3                            -0.549200   \n",
       "4                            -0.521007   \n",
       "\n",
       "   hourly_network-data_fees_fees_total_usd  \\\n",
       "0                                -0.467622   \n",
       "1                                -0.496686   \n",
       "2                                -0.491262   \n",
       "3                                -0.500178   \n",
       "4                                -0.488245   \n",
       "\n",
       "   hourly_market-data_liquidations_htx_global_all_symbol_long_liquidations  \\\n",
       "0                                          -0.289134                         \n",
       "1                                          -0.289134                         \n",
       "2                                          -0.289134                         \n",
       "3                                          -0.289134                         \n",
       "4                                          -0.289134                         \n",
       "\n",
       "   hourly_market-data_liquidations_htx_global_all_symbol_short_liquidations  \\\n",
       "0                                           -0.23536                          \n",
       "1                                           -0.23536                          \n",
       "2                                           -0.23536                          \n",
       "3                                           -0.23536                          \n",
       "4                                           -0.23536                          \n",
       "\n",
       "   hourly_market-data_liquidations_htx_global_all_symbol_long_liquidations_usd  \\\n",
       "0                                            -0.2937                             \n",
       "1                                            -0.2937                             \n",
       "2                                            -0.2937                             \n",
       "3                                            -0.2937                             \n",
       "4                                            -0.2937                             \n",
       "\n",
       "   hourly_market-data_liquidations_htx_global_all_symbol_short_liquidations_usd  \n",
       "0                                          -0.232519                             \n",
       "1                                          -0.232519                             \n",
       "2                                          -0.232519                             \n",
       "3                                          -0.232519                             \n",
       "4                                          -0.232519                             \n",
       "\n",
       "[5 rows x 209 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([std_train_df, std_test_df], ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11552, 20)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델에 사용할 컬럼, 컬럼의 rename rule을 미리 할당함\n",
    "cols_dict: Dict[str, str] = {\n",
    "    \"ID\": \"ID\",\n",
    "    \"target\": \"target\",\n",
    "    \"_type\": \"_type\",\n",
    "    \"hourly_market-data_coinbase-premium-index_coinbase_premium_gap\": \"coinbase_premium_gap\",\n",
    "    \"hourly_market-data_coinbase-premium-index_coinbase_premium_index\": \"coinbase_premium_index\",\n",
    "    \"hourly_market-data_funding-rates_all_exchange_funding_rates\": \"funding_rates\",\n",
    "    \"hourly_market-data_liquidations_all_exchange_all_symbol_long_liquidations\": \"long_liquidations\",\n",
    "    \"hourly_market-data_liquidations_all_exchange_all_symbol_long_liquidations_usd\": \"long_liquidations_usd\",\n",
    "    \"hourly_market-data_liquidations_all_exchange_all_symbol_short_liquidations\": \"short_liquidations\",\n",
    "    \"hourly_market-data_liquidations_all_exchange_all_symbol_short_liquidations_usd\": \"short_liquidations_usd\",\n",
    "    \"hourly_market-data_open-interest_all_exchange_all_symbol_open_interest\": \"open_interest\",\n",
    "    \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_buy_ratio\": \"buy_ratio\",\n",
    "    \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_buy_sell_ratio\": \"buy_sell_ratio\",\n",
    "    \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_buy_volume\": \"buy_volume\",\n",
    "    \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_sell_ratio\": \"sell_ratio\",\n",
    "    \"hourly_market-data_taker-buy-sell-stats_all_exchange_taker_sell_volume\": \"sell_volume\",\n",
    "    \"hourly_network-data_addresses-count_addresses_count_active\": \"active_count\",\n",
    "    \"hourly_network-data_addresses-count_addresses_count_receiver\": \"receiver_count\",\n",
    "    \"hourly_network-data_addresses-count_addresses_count_sender\": \"sender_count\",\n",
    "    \"hourly_market-data_price-ohlcv_all_exchange_spot_btc_usd_close\" : \"close\",\n",
    "}\n",
    "df = df[cols_dict.keys()].rename(cols_dict, axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda 에서 파악한 차이와 차이의 음수, 양수 여부를 새로운 피쳐로 생성\n",
    "df = df.assign(\n",
    "    liquidation_diff=df[\"long_liquidations\"] - df[\"short_liquidations\"],\n",
    "    liquidation_usd_diff=df[\"long_liquidations_usd\"] - df[\"short_liquidations_usd\"],\n",
    "    volume_diff=df[\"buy_volume\"] - df[\"sell_volume\"],\n",
    "    liquidation_diffg=np.sign(df[\"long_liquidations\"] - df[\"short_liquidations\"]),\n",
    "    liquidation_usd_diffg=np.sign(df[\"long_liquidations_usd\"] - df[\"short_liquidations_usd\"]),\n",
    "    volume_diffg=np.sign(df[\"buy_volume\"] - df[\"sell_volume\"]),\n",
    "    buy_sell_volume_ratio=df[\"buy_volume\"] / (df[\"sell_volume\"] + 1),\n",
    "    close_diff = df['close'].diff().fillna(0),\n",
    "    close_diffg = np.sign(df['close'].diff().fillna(0))\n",
    ")\n",
    "# category, continuous 열을 따로 할당해둠\n",
    "category_cols: List[str] = [\"liquidation_diffg\", \"liquidation_usd_diffg\", \"volume_diffg\", \"close_diffg\"]\n",
    "conti_cols: List[str] = [_ for _ in cols_dict.values() if _ not in [\"ID\", \"target\", \"_type\"]] + [\n",
    "    \"buy_sell_volume_ratio\",\n",
    "    \"liquidation_diff\",\n",
    "    \"liquidation_usd_diff\",\n",
    "    \"volume_diff\",\n",
    "    \"close_diff\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_feature(\n",
    "    df: pd.DataFrame,\n",
    "    conti_cols: List[str],\n",
    "    intervals: List[int],\n",
    ") -> List[pd.Series]:\n",
    "    \"\"\"\n",
    "    연속형 변수의 shift feature 생성\n",
    "    Args:\n",
    "        df (pd.DataFrame)\n",
    "        conti_cols (List[str]): continuous colnames\n",
    "        intervals (List[int]): shifted intervals\n",
    "    Return:\n",
    "        List[pd.Series]\n",
    "    \"\"\"\n",
    "    df_shift_dict = [\n",
    "        df[conti_col].shift(interval).rename(f\"{conti_col}_{interval}\")\n",
    "        for conti_col in conti_cols\n",
    "        for interval in intervals\n",
    "    ]\n",
    "    return df_shift_dict\n",
    "\n",
    "# 최대 24시간의 shift 피쳐를 계산\n",
    "shift_list = shift_feature(\n",
    "    df=df, conti_cols=conti_cols, intervals=[_ for _ in range(1, 24)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat 하여 df 에 할당\n",
    "df = pd.concat([df, pd.concat(shift_list, axis=1)], axis=1)\n",
    "\n",
    "# 타겟 변수를 제외한 변수를 forwardfill, -999로 결측치 대체\n",
    "_target = df[\"target\"]\n",
    "df = df.ffill().fillna(-999).assign(target = _target)\n",
    "\n",
    "# _type에 따라 train, test 분리\n",
    "train_df = df.loc[df[\"_type\"]==\"train\"].drop(columns=[\"_type\"])\n",
    "test_df = df.loc[df[\"_type\"]==\"test\"].drop(columns=[\"_type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### patchTST training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### data split & loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final data\n",
    "print(train_df.shape, test_df.shape)\n",
    "\n",
    "# train data split for training\n",
    "train_data = train_df.iloc[:,2:]\n",
    "valid_data = train_df[\"target\"]\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_data, valid_data, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "# 3. 데이터 텐서 변환\n",
    "train_X_tensor = torch.tensor(x_train.values, dtype=torch.float32)\n",
    "train_y_tensor = torch.tensor(y_train.values.squeeze(), dtype=torch.long)\n",
    "valid_X_tensor = torch.tensor(x_valid.values, dtype=torch.float32)\n",
    "valid_y_tensor = torch.tensor(y_valid.values.squeeze(), dtype=torch.long)\n",
    "\n",
    "# 4. 데이터셋과 DataLoader 준비\n",
    "# seq_len 설정\n",
    "seq_len = 24\n",
    "batch_size = 128\n",
    "\n",
    "# 5. 시계열 데이터에 맞게 3차원 텐서로 변환\n",
    "def create_sequences(X, y, seq_len):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(X) - seq_len):\n",
    "        seq = X[i:i + seq_len]\n",
    "        label = y[i + seq_len]  # 다음 시간 스텝의 레이블\n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "    return torch.stack(sequences), torch.tensor(labels)\n",
    "\n",
    "train_X_seq, train_y_seq = create_sequences(train_X_tensor, train_y_tensor, seq_len)\n",
    "valid_X_seq, valid_y_seq = create_sequences(valid_X_tensor, valid_y_tensor, seq_len)\n",
    "\n",
    "# DataLoader 설정\n",
    "train_dataset = TensorDataset(train_X_seq, train_y_seq)\n",
    "valid_dataset = TensorDataset(valid_X_seq, valid_y_seq)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 24, 532])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transpose(nn.Module):\n",
    "    def __init__(self, *dims, contiguous=False): \n",
    "        super().__init__()\n",
    "        self.dims, self.contiguous = dims, contiguous\n",
    "    def forward(self, x):\n",
    "        if self.contiguous: return x.transpose(*self.dims).contiguous()\n",
    "        else: return x.transpose(*self.dims)\n",
    "\n",
    "\n",
    "class FlattenHead(nn.Module):\n",
    "    def __init__(self, n_vars, nf, target_window, head_dropout=0):\n",
    "        super().__init__()\n",
    "        self.n_vars = n_vars\n",
    "        self.flatten = nn.Flatten(start_dim=-2)\n",
    "        self.linear = nn.Linear(nf, target_window)\n",
    "        self.dropout = nn.Dropout(head_dropout)\n",
    "\n",
    "    def forward(self, x):  # x: [bs x nvars x d_model x patch_num]\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, configs, patch_len=24, stride=12):\n",
    "        \"\"\"\n",
    "        patch_len: int, patch len for patch_embedding\n",
    "        stride: int, stride for patch_embedding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.task_name = configs['task_name']\n",
    "        self.seq_len = configs['seq_len']\n",
    "        self.pred_len = configs['pred_len']\n",
    "        padding = stride\n",
    "\n",
    "        # patching and embedding\n",
    "        self.patch_embedding = PatchEmbedding(\n",
    "            configs['d_model'], patch_len, stride, padding, configs['dropout'])\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(False, configs['factor'], attention_dropout=configs['dropout'],\n",
    "                                      output_attention=configs['output_attention']), configs['d_model'], configs['n_heads']),\n",
    "                    configs['d_model'],\n",
    "                    configs['d_ff'],\n",
    "                    dropout=configs['dropout'],\n",
    "                    activation=configs['activation']\n",
    "                ) for l in range(configs['e_layers'])\n",
    "            ],\n",
    "            norm_layer=nn.Sequential(Transpose(1,2), nn.BatchNorm1d(configs['d_model']), Transpose(1,2))\n",
    "        )\n",
    "\n",
    "        # Prediction Head\n",
    "        self.head_nf = configs['d_model'] * \\\n",
    "                       int((configs['seq_len'] - patch_len) / stride + 2)\n",
    "        if self.task_name == 'classification':\n",
    "            self.flatten = nn.Flatten(start_dim=-2)\n",
    "            self.dropout = nn.Dropout(configs['dropout'])\n",
    "            self.projection = nn.Linear(\n",
    "                self.head_nf * configs['enc_in'], configs['num_class'])\n",
    "\n",
    "    def classification(self, x_enc):\n",
    "        # Normalization from Non-stationary Transformer\n",
    "        means = x_enc.mean(1, keepdim=True).detach()\n",
    "        x_enc = x_enc - means\n",
    "        stdev = torch.sqrt(\n",
    "            torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
    "        x_enc /= stdev\n",
    "\n",
    "        # do patching and embedding\n",
    "        x_enc = x_enc.permute(0, 2, 1)\n",
    "        # u: [bs * nvars x patch_num x d_model]\n",
    "        enc_out, n_vars = self.patch_embedding(x_enc)\n",
    "\n",
    "        # Encoder\n",
    "        # z: [bs * nvars x patch_num x d_model]\n",
    "        enc_out, attns = self.encoder(enc_out)\n",
    "        # z: [bs x nvars x patch_num x d_model]\n",
    "        enc_out = torch.reshape(\n",
    "            enc_out, (-1, n_vars, enc_out.shape[-2], enc_out.shape[-1]))\n",
    "        # z: [bs x nvars x d_model x patch_num]\n",
    "        enc_out = enc_out.permute(0, 1, 3, 2)\n",
    "\n",
    "        # Decoder\n",
    "        output = self.flatten(enc_out)\n",
    "        output = self.dropout(output)\n",
    "        output = output.reshape(output.shape[0], -1)\n",
    "        output = self.projection(output)  # (batch_size, num_classes)\n",
    "        return output\n",
    "\n",
    "    def forward(self, x_enc, mask=None):\n",
    "        if self.task_name == 'classification':\n",
    "            dec_out = self.classification(x_enc)\n",
    "            return dec_out  # [B, N]\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    'task_name': 'classification',\n",
    "    'seq_len': 24,  # Total features\n",
    "    'pred_len': 24,  # Prediction length\n",
    "    'd_model': 128,  # Embedding dimension\n",
    "    'dropout': 0.1,\n",
    "    'factor': 5,\n",
    "    'output_attention': False,\n",
    "    'n_heads': 4,  # Number of heads in the Transformer\n",
    "    'e_layers': 3,  # Number of encoder layers\n",
    "    'd_ff': 256,  # Dimension of feed-forward layers\n",
    "    'enc_in': 532,  # Input size\n",
    "    'num_class': 4,  # Number of classes\n",
    "    'activation': 'relu'\n",
    "}\n",
    "\n",
    "model = Model(configs, patch_len=24, stride=12).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 3.7949223041534426\n",
      "Validation Accuracy: 43.11%\n",
      "Validation Loss: 1.7809\n",
      "Epoch [11/50], Loss: 1.1881092104044828\n",
      "Validation Accuracy: 39.41%\n",
      "Validation Loss: 2.7812\n",
      "Epoch [21/50], Loss: 1.0780133962631226\n",
      "Validation Accuracy: 9.43%\n",
      "Validation Loss: 2.4129\n",
      "Epoch [31/50], Loss: 1.0058964317495174\n",
      "Validation Accuracy: 31.60%\n",
      "Validation Loss: 1.5497\n",
      "Epoch [41/50], Loss: 0.9773329442197626\n",
      "Validation Accuracy: 37.67%\n",
      "Validation Loss: 1.4211\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "patience = 5  # Improvement 없을 때 기다릴 에포크 수\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)  # Adjusted based on the model's input\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)  # Validation loss 계산\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Prediction and accuracy calculation\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_loss = val_loss / len(valid_loader)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Validation Accuracy: {accuracy * 100:.2f}%')\n",
    "        print(f'Validation Loss: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2816, 532)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.concat([train_df.iloc[-24:,2:], test_df.iloc[:,2:]], axis=0)\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시퀀스 길이 및 특징 개수 정의\n",
    "seq_len = 24\n",
    "feature_n = 532\n",
    "\n",
    "# 슬라이딩 윈도우로 데이터 준비\n",
    "def create_sequences(X, seq_len):\n",
    "    sequences = []\n",
    "    for i in range(len(X) - seq_len):\n",
    "        seq = X[i:i + seq_len]\n",
    "        sequences.append(seq)\n",
    "    return torch.stack(sequences)\n",
    "\n",
    "# test_X_tensor: (2792, 532)\n",
    "test_X_tensor = torch.tensor(test_data.values, dtype=torch.float32)\n",
    "\n",
    "# 2. seq_len을 24로 설정하고, 시계열 데이터에 맞게 변환\n",
    "seq_len = 24\n",
    "test_X_seq = create_sequences(test_X_tensor, seq_len)\n",
    "\n",
    "# 3. DataLoader 생성 (배치 사이즈는 자유롭게 설정 가능, 여기서는 128)\n",
    "test_dataset = TensorDataset(test_X_seq)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 예측 수행\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "with torch.no_grad():\n",
    "    for inputs in test_loader:\n",
    "        inputs = inputs[0].to('cuda')  # 입력 데이터만 필요\n",
    "        outputs = model(inputs, None)  # 모델의 예측 결과\n",
    "        _, predicted = torch.max(outputs, 1)  # 예측 클래스 선택\n",
    "        test_predictions.extend(predicted.cpu().numpy())  # 결과를 리스트에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output file 할당후 save \n",
    "submission_df = submission_df.assign(target = test_predictions)\n",
    "submission_df.to_csv(\"output/output_patchTST.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transpose(nn.Module):\n",
    "    def __init__(self, *dims, contiguous=False): \n",
    "        super().__init__()\n",
    "        self.dims, self.contiguous = dims, contiguous\n",
    "    def forward(self, x):\n",
    "        if self.contiguous: return x.transpose(*self.dims).contiguous()\n",
    "        else: return x.transpose(*self.dims)\n",
    "\n",
    "\n",
    "class FlattenHead(nn.Module):\n",
    "    def __init__(self, n_vars, nf, target_window, head_dropout=0):\n",
    "        super().__init__()\n",
    "        self.n_vars = n_vars\n",
    "        self.flatten = nn.Flatten(start_dim=-2)\n",
    "        self.linear = nn.Linear(nf, target_window)\n",
    "        self.dropout = nn.Dropout(head_dropout)\n",
    "\n",
    "    def forward(self, x):  # x: [bs x nvars x d_model x patch_num]\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, configs, patch_len=16, stride=8):\n",
    "        super().__init__()\n",
    "        self.seq_len = configs['seq_len']\n",
    "        self.pred_len = configs['pred_len']\n",
    "        padding = stride\n",
    "\n",
    "        # patching and embedding\n",
    "        self.patch_embedding = PatchEmbedding(\n",
    "            configs['d_model'], patch_len, stride, padding, configs['dropout'])\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(False, configs['factor'], attention_dropout=configs['dropout'],\n",
    "                                      output_attention=configs['output_attention']), configs['d_model'], configs['n_heads']),\n",
    "                    configs['d_model'],\n",
    "                    configs['d_ff'],\n",
    "                    dropout=configs['dropout'],\n",
    "                    activation=configs['activation']\n",
    "                ) for l in range(configs['e_layers'])\n",
    "            ],\n",
    "            norm_layer=nn.Sequential(Transpose(1,2), nn.BatchNorm1d(configs['d_model']), Transpose(1,2))\n",
    "        )\n",
    "\n",
    "        # Prediction Head\n",
    "        self.head_nf = configs['d_model'] * \\\n",
    "                       int((configs['seq_len'] - patch_len) / stride + 2)\n",
    "        self.head = FlattenHead(configs['enc_in'], self.head_nf, configs['pred_len'],\n",
    "                                    head_dropout=configs['dropout'])\n",
    "\n",
    "    def forecast(self, x_enc):\n",
    "        # Normalization from Non-stationary Transformer\n",
    "        means = x_enc.mean(1, keepdim=True).detach()\n",
    "        x_enc = x_enc - means\n",
    "        stdev = torch.sqrt(\n",
    "            torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
    "        x_enc /= stdev\n",
    "\n",
    "        # do patching and embedding\n",
    "        x_enc = x_enc.permute(0, 2, 1)\n",
    "        # u: [bs * nvars x patch_num x d_model]\n",
    "        enc_out, n_vars = self.patch_embedding(x_enc)\n",
    "\n",
    "        # Encoder\n",
    "        # z: [bs * nvars x patch_num x d_model]\n",
    "        enc_out, attns = self.encoder(enc_out)\n",
    "        # z: [bs x nvars x patch_num x d_model]\n",
    "        enc_out = torch.reshape(\n",
    "            enc_out, (-1, n_vars, enc_out.shape[-2], enc_out.shape[-1]))\n",
    "        # z: [bs x nvars x d_model x patch_num]\n",
    "        enc_out = enc_out.permute(0, 1, 3, 2)\n",
    "\n",
    "        # Decoder\n",
    "        dec_out = self.head(enc_out)  # z: [bs x nvars x target_window]\n",
    "        dec_out = dec_out.permute(0, 2, 1)\n",
    "\n",
    "        # De-Normalization from Non-stationary Transformer\n",
    "        dec_out = dec_out * \\\n",
    "                  (stdev[:, 0, :].unsqueeze(1).repeat(1, self.pred_len, 1))\n",
    "        dec_out = dec_out + \\\n",
    "                  (means[:, 0, :].unsqueeze(1).repeat(1, self.pred_len, 1))\n",
    "        return dec_out\n",
    "\n",
    "    def forward(self, x_enc):\n",
    "        dec_out = self.forecast(x_enc)\n",
    "        return dec_out[:, -self.pred_len:, :]  # [B, L, D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 타겟과 피처 설정\n",
    "y_train = std_train_df['hourly_market-data_price-ohlcv_all_exchange_spot_btc_usd_close']\n",
    "X_train = std_train_df.drop(columns=['hourly_market-data_price-ohlcv_all_exchange_spot_btc_usd_close', 'ID', 'target', '_type'], errors='ignore')\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "# 3. 데이터 텐서 변환\n",
    "train_X_tensor = torch.tensor(x_train.values, dtype=torch.float32)\n",
    "train_y_tensor = torch.tensor(y_train.values.squeeze(), dtype=torch.long)\n",
    "valid_X_tensor = torch.tensor(x_valid.values, dtype=torch.float32)\n",
    "valid_y_tensor = torch.tensor(y_valid.values.squeeze(), dtype=torch.long)\n",
    "\n",
    "# 4. 데이터셋과 DataLoader 준비\n",
    "# seq_len 설정\n",
    "seq_len = 24\n",
    "batch_size = 128\n",
    "\n",
    "# 데이터 텐서 변환\n",
    "train_X_tensor = torch.tensor(x_train.values, dtype=torch.float32)\n",
    "train_y_tensor = torch.tensor(y_train.values.squeeze(), dtype=torch.float32)  # Change dtype to float32 for regression\n",
    "valid_X_tensor = torch.tensor(x_valid.values, dtype=torch.float32)\n",
    "valid_y_tensor = torch.tensor(y_valid.values.squeeze(), dtype=torch.float32)  # Change dtype to float32 for regression\n",
    "\n",
    "# 시계열 데이터에 맞게 3차원 텐서로 변환\n",
    "def create_sequences(X, y, seq_len):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(X) - seq_len):\n",
    "        seq = X[i:i + seq_len]\n",
    "        label = y[i + seq_len]  # 다음 시간 스텝의 레이블\n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "    return torch.stack(sequences), torch.stack(labels)\n",
    "\n",
    "train_X_seq, train_y_seq = create_sequences(train_X_tensor, train_y_tensor, seq_len)\n",
    "valid_X_seq, valid_y_seq = create_sequences(valid_X_tensor, valid_y_tensor, seq_len)\n",
    "\n",
    "# DataLoader 설정\n",
    "train_dataset = TensorDataset(train_X_seq, train_y_seq)\n",
    "valid_dataset = TensorDataset(valid_X_seq, valid_y_seq)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    'seq_len': 24,        # 입력 시퀀스 길이\n",
    "    'pred_len': 1,        # 예측할 길이 (가격 예측이므로 1)\n",
    "    'enc_in': 205,        # 입력 피처 수\n",
    "    'd_model': 64,        # 모델 차원\n",
    "    'e_layers': 3,        # 인코더 레이어 수\n",
    "    'd_ff': 128,          # 피드 포워드 네트워크 차원\n",
    "    'n_heads': 4,         # 어텐션 헤드 수\n",
    "    'factor': 5,          # 어텐션 팩터 (적절히 설정)\n",
    "    'dropout': 0.1,       # 드롭아웃 비율\n",
    "    'activation': 'relu',  # 활성화 함수\n",
    "    'output_attention': False # 어텐션 출력 여부\n",
    "}\n",
    "\n",
    "model = Model(configs, patch_len=24, stride=12).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([128, 24, 205])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 1, 205])\n",
      "torch.Size([128, 1])\n",
      "1\n",
      "torch.Size([128, 24, 205])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 1, 205])\n",
      "torch.Size([128, 1])\n",
      "2\n",
      "torch.Size([128, 24, 205])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 1, 205])\n",
      "torch.Size([128, 1])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (x_enc, y) in enumerate(train_loader):\n",
    "    print(batch_idx)\n",
    "    print(x_enc.shape)\n",
    "    print(y.shape)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(x_enc.to(device))\n",
    "\n",
    "    a = outputs\n",
    "    b = y.to(device).unsqueeze(1)\n",
    "    print(a.shape)\n",
    "    print(b.shape)\n",
    "    \n",
    "    if batch_idx > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train Loss: 0.8269, Validation Loss: 3.0366\n",
      "Epoch [2/5], Train Loss: 0.8243, Validation Loss: 3.0350\n",
      "Epoch [3/5], Train Loss: 0.8312, Validation Loss: 3.0363\n",
      "Epoch [4/5], Train Loss: 0.8262, Validation Loss: 3.0355\n",
      "Epoch [5/5], Train Loss: 0.8274, Validation Loss: 3.0352\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "num_epochs = 5  # Set number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for batch_idx, (x_enc, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()  # Zero gradients\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(x_enc.to(device))  # Model's prediction\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs[:, -1, :], y.to(device).unsqueeze(1))  # Compare the last prediction with target\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    # Validation Loop\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_valid_loss = 0\n",
    "\n",
    "    with torch.no_grad():  # No need to compute gradients during validation\n",
    "        for batch_idx, (x_enc, y) in enumerate(valid_loader):\n",
    "            outputs = model(x_enc.to(device))  # Model's prediction\n",
    "            loss = criterion(outputs[:, -1, :], y.to(device).unsqueeze(1))  # Compare the last prediction with target\n",
    "            total_valid_loss += loss.item()\n",
    "\n",
    "    avg_valid_loss = total_valid_loss / len(valid_loader)\n",
    "\n",
    "    # Print epoch statistics\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "          f'Train Loss: {avg_train_loss:.4f}, '\n",
    "          f'Validation Loss: {avg_valid_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = std_test_df.drop(columns=['hourly_market-data_price-ohlcv_all_exchange_spot_btc_usd_close', 'ID', 'target', '_type'], errors='ignore')\n",
    "\n",
    "test_X_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "\n",
    "def create_sequences(X, seq_len):\n",
    "    sequences = []\n",
    "    for i in range(len(X) - seq_len):\n",
    "        seq = X[i:i + seq_len]\n",
    "        sequences.append(seq)\n",
    "    return torch.stack(sequences)\n",
    "\n",
    "test_X_seq = create_sequences(test_X_tensor, seq_len)\n",
    "\n",
    "test_dataset = TensorDataset(test_X_seq)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():  # No need to compute gradients during testing\n",
    "    for x_enc in test_loader:\n",
    "        x_enc = x_enc[0]  # Get the sequence tensor from the DataLoader\n",
    "        if torch.cuda.is_available():  # Check if CUDA is available\n",
    "            x_enc = x_enc.cuda()  # Move to GPU if available\n",
    "        output = model(x_enc)  # Get model predictions\n",
    "        predictions.append(outputs[:, -1, :])  # Append the entire output\n",
    "\n",
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "predictions.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Concatenate the predictions into a single tensor\n",
    "predictions_tensor = torch.cat(predictions, dim=0)\n",
    "\n",
    "# Move predictions to CPU before converting to numpy\n",
    "predictions_array = predictions_tensor.cpu().numpy()\n",
    "\n",
    "# Check the shape of predictions\n",
    "print(predictions_array.shape)  # This should match [N, D] where N is the number of predictions\n",
    "\n",
    "# If you only want the last time step of predictions, you can use:\n",
    "final_predictions = predictions_array.squeeze(axis=1)  # Get the last time step's predictions\n",
    "\n",
    "# Output the final predictions\n",
    "print(final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      -2.083215\n",
       "1      -2.079750\n",
       "2      -2.081277\n",
       "3      -2.083744\n",
       "4      -2.085260\n",
       "          ...   \n",
       "8755    2.344851\n",
       "8756    2.334293\n",
       "8757    2.327828\n",
       "8758    2.286965\n",
       "8759    2.291854\n",
       "Name: hourly_market-data_price-ohlcv_all_exchange_spot_btc_usd_close, Length: 8760, dtype: float64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = std_train_df['hourly_market-data_price-ohlcv_all_exchange_spot_btc_usd_close']\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.899141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.021093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.031150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.049061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.287036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567435</th>\n",
       "      <td>1.047306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567436</th>\n",
       "      <td>-0.107387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567437</th>\n",
       "      <td>-0.214363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567438</th>\n",
       "      <td>0.118492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567439</th>\n",
       "      <td>-0.189165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>567440 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "0       0.899141\n",
       "1      -0.021093\n",
       "2      -0.031150\n",
       "3       0.049061\n",
       "4       0.287036\n",
       "...          ...\n",
       "567435  1.047306\n",
       "567436 -0.107387\n",
       "567437 -0.214363\n",
       "567438  0.118492\n",
       "567439 -0.189165\n",
       "\n",
       "[567440 rows x 1 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = pd.DataFrame(final_predictions)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (567439,) and (567440,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(y_train\u001b[38;5;241m.\u001b[39mindex, y_train\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_predictions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_predictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/matplotlib/pyplot.py:3794\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3786\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   3787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\n\u001b[1;32m   3788\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3793\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[0;32m-> 3794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3795\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3798\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3799\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3800\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/matplotlib/axes/_axes.py:1779\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1776\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1777\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1778\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1779\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/matplotlib/axes/_base.py:296\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    295\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/matplotlib/axes/_base.py:486\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    483\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 486\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    487\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    490\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (567439,) and (567440,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(y_train.index, y_train.values)\n",
    "plt.plot(range(y_train.shape[0]+1, y_train.shape[0]+len(final_predictions)), final_predictions, c = 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dlinear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, configs, individual=False):\n",
    "        super(Model, self).__init__()\n",
    "        self.seq_len = configs['seq_len']\n",
    "        self.pred_len = configs['seq_len']\n",
    "        # Series decomposition block from Autoformer\n",
    "        self.decompsition = series_decomp(configs['moving_avg'])\n",
    "        self.individual = individual\n",
    "        self.channels = configs['enc_in']\n",
    "\n",
    "        if self.individual:\n",
    "            self.Linear_Seasonal = nn.ModuleList()\n",
    "            self.Linear_Trend = nn.ModuleList()\n",
    "\n",
    "            for i in range(self.channels):\n",
    "                self.Linear_Seasonal.append(\n",
    "                    nn.Linear(self.seq_len, self.pred_len))\n",
    "                self.Linear_Trend.append(\n",
    "                    nn.Linear(self.seq_len, self.pred_len))\n",
    "\n",
    "                self.Linear_Seasonal[i].weight = nn.Parameter(\n",
    "                    (1 / self.seq_len) * torch.ones([self.pred_len, self.seq_len]))\n",
    "                self.Linear_Trend[i].weight = nn.Parameter(\n",
    "                    (1 / self.seq_len) * torch.ones([self.pred_len, self.seq_len]))\n",
    "        else:\n",
    "            self.Linear_Seasonal = nn.Linear(self.seq_len, self.pred_len)\n",
    "            self.Linear_Trend = nn.Linear(self.seq_len, self.pred_len)\n",
    "\n",
    "            self.Linear_Seasonal.weight = nn.Parameter(\n",
    "                (1 / self.seq_len) * torch.ones([self.pred_len, self.seq_len]))\n",
    "            self.Linear_Trend.weight = nn.Parameter(\n",
    "                (1 / self.seq_len) * torch.ones([self.pred_len, self.seq_len]))\n",
    "\n",
    "        self.projection = nn.Linear(\n",
    "            configs['enc_in'] * configs['seq_len'], configs['num_class'])\n",
    "\n",
    "    def encoder(self, x):\n",
    "        seasonal_init, trend_init = self.decompsition(x)\n",
    "        seasonal_init, trend_init = seasonal_init.permute(\n",
    "            0, 2, 1), trend_init.permute(0, 2, 1)\n",
    "        if self.individual:\n",
    "            seasonal_output = torch.zeros([seasonal_init.size(0), seasonal_init.size(1), self.pred_len],\n",
    "                                          dtype=seasonal_init.dtype).to(seasonal_init.device)\n",
    "            trend_output = torch.zeros([trend_init.size(0), trend_init.size(1), self.pred_len],\n",
    "                                       dtype=trend_init.dtype).to(trend_init.device)\n",
    "            for i in range(self.channels):\n",
    "                seasonal_output[:, i, :] = self.Linear_Seasonal[i](\n",
    "                    seasonal_init[:, i, :])\n",
    "                trend_output[:, i, :] = self.Linear_Trend[i](\n",
    "                    trend_init[:, i, :])\n",
    "        else:\n",
    "            seasonal_output = self.Linear_Seasonal(seasonal_init)\n",
    "            trend_output = self.Linear_Trend(trend_init)\n",
    "        x = seasonal_output + trend_output\n",
    "        return x.permute(0, 2, 1)\n",
    "\n",
    "    def classification(self, x_enc):\n",
    "        # Encoder\n",
    "        enc_out = self.encoder(x_enc)\n",
    "        # Output\n",
    "        # (batch_size, seq_length * d_model)\n",
    "        output = enc_out.reshape(enc_out.shape[0], -1)\n",
    "        # (batch_size, num_classes)\n",
    "        output = self.projection(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, x_enc, mask=None):\n",
    "        dec_out = self.classification(x_enc)\n",
    "        return dec_out  # [B, N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data split for training\n",
    "train_data = train_df.iloc[:,2:]\n",
    "valid_data = train_df[\"target\"]\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_data, valid_data, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "# 3. 데이터 텐서 변환\n",
    "train_X_tensor = torch.tensor(x_train.values, dtype=torch.float32)\n",
    "train_y_tensor = torch.tensor(y_train.values.squeeze(), dtype=torch.long)\n",
    "valid_X_tensor = torch.tensor(x_valid.values, dtype=torch.float32)\n",
    "valid_y_tensor = torch.tensor(y_valid.values.squeeze(), dtype=torch.long)\n",
    "\n",
    "# 4. 데이터셋과 DataLoader 준비\n",
    "# seq_len 설정\n",
    "seq_len = 24\n",
    "batch_size = 64\n",
    "\n",
    "# 5. 시계열 데이터에 맞게 3차원 텐서로 변환\n",
    "def create_sequences(X, y, seq_len):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(X) - seq_len):\n",
    "        seq = X[i:i + seq_len]\n",
    "        label = y[i + seq_len]  # 다음 시간 스텝의 레이블\n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "    return torch.stack(sequences), torch.tensor(labels)\n",
    "\n",
    "train_X_seq, train_y_seq = create_sequences(train_X_tensor, train_y_tensor, seq_len)\n",
    "valid_X_seq, valid_y_seq = create_sequences(valid_X_tensor, valid_y_tensor, seq_len)\n",
    "\n",
    "# DataLoader 설정\n",
    "train_dataset = TensorDataset(train_X_seq, train_y_seq)\n",
    "valid_dataset = TensorDataset(valid_X_seq, valid_y_seq)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 모델 학습 및 검증\n",
    "configs = {\n",
    "    'seq_len': seq_len,\n",
    "    'moving_avg': 12, \n",
    "    'enc_in': x_train.shape[1],  # 입력 특성의 수\n",
    "    'num_class': 4  # 클래스 수\n",
    "}\n",
    "\n",
    "# 모델 생성\n",
    "model = Model(configs=configs, individual=False).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (24) must match the size of tensor b (23) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjusted based on the model's input\u001b[39;00m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[44], line 69\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x_enc, mask)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_enc, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 69\u001b[0m     dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dec_out\n",
      "Cell \u001b[0;32mIn[44], line 60\u001b[0m, in \u001b[0;36mModel.classification\u001b[0;34m(self, x_enc)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclassification\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_enc):\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# Encoder\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     enc_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# Output\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# (batch_size, seq_length * d_model)\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     output \u001b[38;5;241m=\u001b[39m enc_out\u001b[38;5;241m.\u001b[39mreshape(enc_out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[44], line 39\u001b[0m, in \u001b[0;36mModel.encoder\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencoder\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 39\u001b[0m     seasonal_init, trend_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecompsition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     seasonal_init, trend_init \u001b[38;5;241m=\u001b[39m seasonal_init\u001b[38;5;241m.\u001b[39mpermute(\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m), trend_init\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindividual:\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/level1_project/code/test/layers/autoformer_encdec.py:52\u001b[0m, in \u001b[0;36mseries_decomp.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     51\u001b[0m     moving_mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoving_avg(x)\n\u001b[0;32m---> 52\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmoving_mean\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res, moving_mean\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (24) must match the size of tensor b (23) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "patience = 5  # Improvement 없을 때 기다릴 에포크 수\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)  # Adjusted based on the model's input\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)  # Validation loss 계산\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Prediction and accuracy calculation\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_loss = val_loss / len(valid_loader)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Validation Accuracy: {accuracy * 100:.2f}%')\n",
    "        print(f'Validation Loss: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SegRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, configs):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # get parameters\n",
    "        self.seq_len = configs['seq_len']\n",
    "        self.enc_in = configs['enc_in']\n",
    "        self.d_model = configs['d_model']\n",
    "        self.dropout = configs['dropout']\n",
    "\n",
    "        self.pred_len = configs['seq_len']\n",
    "\n",
    "        self.seg_len = configs['seg_len']\n",
    "        self.seg_num_x = self.seq_len // self.seg_len\n",
    "        self.seg_num_y = self.pred_len // self.seg_len\n",
    "\n",
    "        # building model\n",
    "        self.valueEmbedding = nn.Sequential(\n",
    "            nn.Linear(self.seg_len, self.d_model),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.rnn = nn.GRU(input_size=self.d_model, hidden_size=self.d_model, num_layers=1, bias=True,\n",
    "                              batch_first=True, bidirectional=False)\n",
    "        self.pos_emb = nn.Parameter(torch.randn(self.seg_num_y, self.d_model // 2))\n",
    "        self.channel_emb = nn.Parameter(torch.randn(self.enc_in, self.d_model // 2))\n",
    "\n",
    "        self.predict = nn.Sequential(\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.d_model, self.seg_len)\n",
    "        )\n",
    "\n",
    "        self.act = F.gelu\n",
    "        self.dropout = nn.Dropout(configs['dropout'])\n",
    "        self.projection = nn.Linear(\n",
    "            configs['enc_in'] * configs['seq_len'], configs['num_class'])\n",
    "\n",
    "    def encoder(self, x):\n",
    "        # b:batch_size c:channel_size s:seq_len s:seq_len\n",
    "        # d:d_model w:seg_len n:seg_num_x m:seg_num_y\n",
    "        batch_size = x.size(0)\n",
    "        seq_last = x[:, -1:, :].detach()\n",
    "        x = (x - seq_last).permute(0, 2, 1) # b,c,s\n",
    "\n",
    "        x = self.valueEmbedding(x.reshape(-1, self.seg_num_x, self.seg_len))\n",
    "\n",
    "        # encoding\n",
    "        _, hn = self.rnn(x) # bc,n,d  1,bc,d\n",
    "\n",
    "        # m,d//2 -> 1,m,d//2 -> c,m,d//2\n",
    "        # c,d//2 -> c,1,d//2 -> c,m,d//2\n",
    "        # c,m,d -> cm,1,d -> bcm, 1, d\n",
    "        pos_emb = torch.cat([\n",
    "            self.pos_emb.unsqueeze(0).repeat(self.enc_in, 1, 1),\n",
    "            self.channel_emb.unsqueeze(1).repeat(1, self.seg_num_y, 1)\n",
    "        ], dim=-1).view(-1, 1, self.d_model).repeat(batch_size,1,1)\n",
    "\n",
    "        _, hy = self.rnn(pos_emb, hn.repeat(1, 1, self.seg_num_y).view(1, -1, self.d_model)) # bcm,1,d  1,bcm,d\n",
    "\n",
    "        # 1,bcm,d -> 1,bcm,w -> b,c,s\n",
    "        y = self.predict(hy).view(-1, self.enc_in, self.pred_len)\n",
    "\n",
    "        # permute and denorm\n",
    "        y = y.permute(0, 2, 1) + seq_last\n",
    "        return y\n",
    "\n",
    "    def classification(self, x_enc):\n",
    "        # Encoder\n",
    "        enc_out = self.encoder(x_enc)\n",
    "        # Output\n",
    "        # (batch_size, seq_length * d_model)\n",
    "        output = enc_out.reshape(enc_out.shape[0], -1)\n",
    "        # (batch_size, num_classes)\n",
    "        output = self.projection(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, x_enc):\n",
    "        dec_out = self.classification(x_enc)\n",
    "        return dec_out  # [B, N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data split for training\n",
    "train_data = train_df.iloc[:,2:]\n",
    "valid_data = train_df[\"target\"]\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_data, valid_data, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "# 3. 데이터 텐서 변환\n",
    "train_X_tensor = torch.tensor(x_train.values, dtype=torch.float32)\n",
    "train_y_tensor = torch.tensor(y_train.values.squeeze(), dtype=torch.long)\n",
    "valid_X_tensor = torch.tensor(x_valid.values, dtype=torch.float32)\n",
    "valid_y_tensor = torch.tensor(y_valid.values.squeeze(), dtype=torch.long)\n",
    "\n",
    "# 4. 데이터셋과 DataLoader 준비\n",
    "# seq_len 설정\n",
    "seq_len = 24\n",
    "batch_size = 128\n",
    "\n",
    "# 5. 시계열 데이터에 맞게 3차원 텐서로 변환\n",
    "def create_sequences(X, y, seq_len):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(X) - seq_len):\n",
    "        seq = X[i:i + seq_len]\n",
    "        label = y[i + seq_len]  # 다음 시간 스텝의 레이블\n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "    return torch.stack(sequences), torch.tensor(labels)\n",
    "\n",
    "train_X_seq, train_y_seq = create_sequences(train_X_tensor, train_y_tensor, seq_len)\n",
    "valid_X_seq, valid_y_seq = create_sequences(valid_X_tensor, valid_y_tensor, seq_len)\n",
    "\n",
    "# DataLoader 설정\n",
    "train_dataset = TensorDataset(train_X_seq, train_y_seq)\n",
    "valid_dataset = TensorDataset(valid_X_seq, valid_y_seq)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 24, 532])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "configs = {\n",
    "    'seq_len': 24,    # For example, define your sequence length\n",
    "    'enc_in': 532,    # Input feature dimension\n",
    "    'd_model': 128,    # Model dimension\n",
    "    'dropout': 0.1,   # Dropout rate\n",
    "    'seg_len': 6,     # Define segment length\n",
    "    'num_class': 4    # Number of classes for classification\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = Model(configs)\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Test Accuracy: 40.05%\n",
      "Epoch [2/10], Test Accuracy: 38.54%\n",
      "Epoch [3/10], Test Accuracy: 26.04%\n",
      "Epoch [4/10], Test Accuracy: 28.59%\n",
      "Epoch [5/10], Test Accuracy: 40.51%\n",
      "Epoch [6/10], Test Accuracy: 11.34%\n",
      "Epoch [7/10], Test Accuracy: 21.30%\n",
      "Epoch [8/10], Test Accuracy: 11.81%\n",
      "Epoch [9/10], Test Accuracy: 11.11%\n",
      "Epoch [10/10], Test Accuracy: 13.14%\n"
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Calculate loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update parameters\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, valid_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Get the class with the highest score\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 10  # Number of epochs to train\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, train_loader, criterion, optimizer, device)\n",
    "    accuracy = evaluate(model, valid_loader, device)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Test Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timemixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFT_series_decomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, top_k=5):\n",
    "        super(DFT_series_decomp, self).__init__()\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def forward(self, x):\n",
    "        xf = torch.fft.rfft(x)\n",
    "        freq = abs(xf)\n",
    "        freq[0] = 0\n",
    "        top_k_freq, top_list = torch.topk(freq, 5)\n",
    "        xf[freq <= top_k_freq.min()] = 0\n",
    "        x_season = torch.fft.irfft(xf)\n",
    "        x_trend = x - x_season\n",
    "        return x_season, x_trend\n",
    "\n",
    "\n",
    "class MultiScaleSeasonMixing(nn.Module):\n",
    "    \"\"\"\n",
    "    Bottom-up mixing season pattern\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, configs):\n",
    "        super(MultiScaleSeasonMixing, self).__init__()\n",
    "\n",
    "        self.down_sampling_layers = torch.nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    torch.nn.Linear(\n",
    "                        configs['seq_len'] // (configs['down_sampling_window'] ** i),\n",
    "                        configs['seq_len'] // (configs['down_sampling_window'] ** (i + 1)),\n",
    "                    ),\n",
    "                    nn.GELU(),\n",
    "                    torch.nn.Linear(\n",
    "                        configs['seq_len'] // (configs['down_sampling_window'] ** (i + 1)),\n",
    "                        configs['seq_len'] // (configs['down_sampling_window'] ** (i + 1)),\n",
    "                    ),\n",
    "\n",
    "                )\n",
    "                for i in range(configs['down_sampling_layers'])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, season_list):\n",
    "\n",
    "        # mixing high->low\n",
    "        out_high = season_list[0]\n",
    "        out_low = season_list[1]\n",
    "        out_season_list = [out_high.permute(0, 2, 1)]\n",
    "\n",
    "        for i in range(len(season_list) - 1):\n",
    "            out_low_res = self.down_sampling_layers[i](out_high)\n",
    "            out_low = out_low + out_low_res\n",
    "            out_high = out_low\n",
    "            if i + 2 <= len(season_list) - 1:\n",
    "                out_low = season_list[i + 2]\n",
    "            out_season_list.append(out_high.permute(0, 2, 1))\n",
    "\n",
    "        return out_season_list\n",
    "\n",
    "\n",
    "class MultiScaleTrendMixing(nn.Module):\n",
    "    \"\"\"\n",
    "    Top-down mixing trend pattern\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, configs):\n",
    "        super(MultiScaleTrendMixing, self).__init__()\n",
    "\n",
    "        self.up_sampling_layers = torch.nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    torch.nn.Linear(\n",
    "                        configs['seq_len'] // (configs['down_sampling_window'] ** (i + 1)),\n",
    "                        configs['seq_len'] // (configs['down_sampling_window'] ** i),\n",
    "                    ),\n",
    "                    nn.GELU(),\n",
    "                    torch.nn.Linear(\n",
    "                        configs['seq_len'] // (configs['down_sampling_window'] ** i),\n",
    "                        configs['seq_len'] // (configs['down_sampling_window'] ** i),\n",
    "                    ),\n",
    "                )\n",
    "                for i in reversed(range(configs['down_sampling_layers']))\n",
    "            ])\n",
    "\n",
    "    def forward(self, trend_list):\n",
    "\n",
    "        # mixing low->high\n",
    "        trend_list_reverse = trend_list.copy()\n",
    "        trend_list_reverse.reverse()\n",
    "        out_low = trend_list_reverse[0]\n",
    "        out_high = trend_list_reverse[1]\n",
    "        out_trend_list = [out_low.permute(0, 2, 1)]\n",
    "\n",
    "        for i in range(len(trend_list_reverse) - 1):\n",
    "            out_high_res = self.up_sampling_layers[i](out_low)\n",
    "            out_high = out_high + out_high_res\n",
    "            out_low = out_high\n",
    "            if i + 2 <= len(trend_list_reverse) - 1:\n",
    "                out_high = trend_list_reverse[i + 2]\n",
    "            out_trend_list.append(out_low.permute(0, 2, 1))\n",
    "\n",
    "        out_trend_list.reverse()\n",
    "        return out_trend_list\n",
    "\n",
    "\n",
    "class PastDecomposableMixing(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        super(PastDecomposableMixing, self).__init__()\n",
    "        self.seq_len = configs['seq_len']\n",
    "        self.pred_len = configs['pred_len']\n",
    "        self.down_sampling_window = configs['down_sampling_window']\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(configs['d_model'])\n",
    "        self.dropout = nn.Dropout(configs['dropout'])\n",
    "        self.channel_independence = configs['channel_independence']\n",
    "\n",
    "        if configs['decomp_method'] == 'moving_avg':\n",
    "            self.decompsition = series_decomp(configs['moving_avg'])\n",
    "        elif configs['decomp_method'] == \"dft_decomp\":\n",
    "            self.decompsition = DFT_series_decomp(configs['top_k'])\n",
    "        else:\n",
    "            raise ValueError('decompsition is error')\n",
    "\n",
    "        if not configs['channel_independence']:\n",
    "            self.cross_layer = nn.Sequential(\n",
    "                nn.Linear(in_features=configs['d_model'], out_features=configs['d_ff']),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(in_features=configs['d_ff'], out_features=configs['d_model']),\n",
    "            )\n",
    "\n",
    "        # Mixing season\n",
    "        self.mixing_multi_scale_season = MultiScaleSeasonMixing(configs)\n",
    "\n",
    "        # Mxing trend\n",
    "        self.mixing_multi_scale_trend = MultiScaleTrendMixing(configs)\n",
    "\n",
    "        self.out_cross_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=configs['d_model'], out_features=configs['d_ff']),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_features=configs['d_ff'], out_features=configs['d_model']),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_list):\n",
    "        length_list = []\n",
    "        for x in x_list:\n",
    "            _, T, _ = x.size()\n",
    "            length_list.append(T)\n",
    "\n",
    "        # Decompose to obtain the season and trend\n",
    "        season_list = []\n",
    "        trend_list = []\n",
    "        for x in x_list:\n",
    "            season, trend = self.decompsition(x)\n",
    "            if not self.channel_independence:\n",
    "                season = self.cross_layer(season)\n",
    "                trend = self.cross_layer(trend)\n",
    "            season_list.append(season.permute(0, 2, 1))\n",
    "            trend_list.append(trend.permute(0, 2, 1))\n",
    "\n",
    "        # bottom-up season mixing\n",
    "        out_season_list = self.mixing_multi_scale_season(season_list)\n",
    "        # top-down trend mixing\n",
    "        out_trend_list = self.mixing_multi_scale_trend(trend_list)\n",
    "\n",
    "        out_list = []\n",
    "        for ori, out_season, out_trend, length in zip(x_list, out_season_list, out_trend_list,\n",
    "                                                      length_list):\n",
    "            out = out_season + out_trend\n",
    "            if self.channel_independence:\n",
    "                out = ori + self.out_cross_layer(out)\n",
    "            out_list.append(out[:, :length, :])\n",
    "        return out_list\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, configs):\n",
    "        super(Model, self).__init__()\n",
    "        self.configs = configs\n",
    "        self.seq_len = configs['seq_len']\n",
    "        self.label_len = configs['label_len']\n",
    "        self.pred_len = configs['pred_len']\n",
    "        self.down_sampling_window = configs['down_sampling_window']\n",
    "        self.channel_independence = configs['channel_independence']\n",
    "        self.pdm_blocks = nn.ModuleList([PastDecomposableMixing(configs)\n",
    "                                         for _ in range(configs['e_layers'])])\n",
    "\n",
    "        self.preprocess = series_decomp(configs['moving_avg'])\n",
    "        self.enc_in = configs['enc_in']\n",
    "\n",
    "        if self.channel_independence:\n",
    "            self.enc_embedding = DataEmbedding_wo_pos(1, configs['d_model'], configs['embed'], configs['freq'],\n",
    "                                                      configs['dropout'])\n",
    "        else:\n",
    "            self.enc_embedding = DataEmbedding_wo_pos(configs['enc_in'], configs['d_model'], configs['embed'], configs['freq'],\n",
    "                                                      configs['dropout'])\n",
    "\n",
    "        self.layer = configs['e_layers']\n",
    "\n",
    "        self.normalize_layers = torch.nn.ModuleList(\n",
    "            [\n",
    "                Normalize(self.configs['enc_in'], affine=True, non_norm=True if configs['use_norm'] == 0 else False)\n",
    "                for i in range(configs['down_sampling_layers'] + 1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.act = F.gelu\n",
    "        self.dropout = nn.Dropout(configs['dropout'])\n",
    "        self.projection = nn.Linear(\n",
    "            configs['d_model'] * configs['seq_len'], configs['num_class'])\n",
    "\n",
    "    def out_projection(self, dec_out, i, out_res):\n",
    "        dec_out = self.projection_layer(dec_out)\n",
    "        out_res = out_res.permute(0, 2, 1)\n",
    "        out_res = self.out_res_layers[i](out_res)\n",
    "        out_res = self.regression_layers[i](out_res).permute(0, 2, 1)\n",
    "        dec_out = dec_out + out_res\n",
    "        return dec_out\n",
    "\n",
    "    def pre_enc(self, x_list):\n",
    "        if self.channel_independence:\n",
    "            return (x_list, None)\n",
    "        else:\n",
    "            out1_list = []\n",
    "            out2_list = []\n",
    "            for x in x_list:\n",
    "                x_1, x_2 = self.preprocess(x)\n",
    "                out1_list.append(x_1)\n",
    "                out2_list.append(x_2)\n",
    "            return (out1_list, out2_list)\n",
    "\n",
    "    def __multi_scale_process_inputs(self, x_enc, x_mark_enc):\n",
    "        if self.configs['down_sampling_method'] == 'max':\n",
    "            down_pool = torch.nn.MaxPool1d(self.configs['down_sampling_window'], return_indices=False)\n",
    "        elif self.configs['down_sampling_method'] == 'avg':\n",
    "            down_pool = torch.nn.AvgPool1d(self.configs['down_sampling_window'])\n",
    "        elif self.configs['down_sampling_method'] == 'conv':\n",
    "            padding = 1 if torch.__version__ >= '1.5.0' else 2\n",
    "            down_pool = nn.Conv1d(in_channels=self.configs['enc_in'], out_channels=self.configs['enc_in'],\n",
    "                                  kernel_size=3, padding=padding,\n",
    "                                  stride=self.configs['down_sampling_window'],\n",
    "                                  padding_mode='circular',\n",
    "                                  bias=False)\n",
    "        else:\n",
    "            return x_enc, x_mark_enc\n",
    "        # B,T,C -> B,C,T\n",
    "        x_enc = x_enc.permute(0, 2, 1)\n",
    "\n",
    "        x_enc_ori = x_enc\n",
    "        x_mark_enc_mark_ori = x_mark_enc\n",
    "\n",
    "        x_enc_sampling_list = []\n",
    "        x_mark_sampling_list = []\n",
    "        x_enc_sampling_list.append(x_enc.permute(0, 2, 1))\n",
    "        x_mark_sampling_list.append(x_mark_enc)\n",
    "\n",
    "        for i in range(self.configs['down_sampling_layers']):\n",
    "            x_enc_sampling = down_pool(x_enc_ori)\n",
    "\n",
    "            x_enc_sampling_list.append(x_enc_sampling.permute(0, 2, 1))\n",
    "            x_enc_ori = x_enc_sampling\n",
    "\n",
    "            if x_mark_enc is not None:\n",
    "                x_mark_sampling_list.append(x_mark_enc_mark_ori[:, ::self.configs['down_sampling_window'], :])\n",
    "                x_mark_enc_mark_ori = x_mark_enc_mark_ori[:, ::self.configs['down_sampling_window'], :]\n",
    "\n",
    "        x_enc = x_enc_sampling_list\n",
    "        x_mark_enc = x_mark_sampling_list if x_mark_enc is not None else None\n",
    "\n",
    "        return x_enc, x_mark_enc\n",
    "\n",
    "\n",
    "    def future_multi_mixing(self, B, enc_out_list, x_list):\n",
    "        dec_out_list = []\n",
    "        if self.channel_independence:\n",
    "            x_list = x_list[0]\n",
    "            for i, enc_out in zip(range(len(x_list)), enc_out_list):\n",
    "                dec_out = self.predict_layers[i](enc_out.permute(0, 2, 1)).permute(\n",
    "                    0, 2, 1)  # align temporal dimension\n",
    "                dec_out = self.projection_layer(dec_out)\n",
    "                dec_out = dec_out.reshape(B, self.configs.c_out, self.pred_len).permute(0, 2, 1).contiguous()\n",
    "                dec_out_list.append(dec_out)\n",
    "\n",
    "        else:\n",
    "            for i, enc_out, out_res in zip(range(len(x_list[0])), enc_out_list, x_list[1]):\n",
    "                dec_out = self.predict_layers[i](enc_out.permute(0, 2, 1)).permute(\n",
    "                    0, 2, 1)  # align temporal dimension\n",
    "                dec_out = self.out_projection(dec_out, i, out_res)\n",
    "                dec_out_list.append(dec_out)\n",
    "\n",
    "        return dec_out_list\n",
    "\n",
    "    def classification(self, x_enc):\n",
    "        x_enc, _ = self.__multi_scale_process_inputs(x_enc, None)\n",
    "        x_list = x_enc\n",
    "\n",
    "        # embedding\n",
    "        enc_out_list = []\n",
    "        for x in x_list:\n",
    "            enc_out = self.enc_embedding(x, None)  # [B,T,C]\n",
    "            enc_out_list.append(enc_out)\n",
    "\n",
    "        # MultiScale-CrissCrossAttention  as encoder for past\n",
    "        for i in range(self.layer):\n",
    "            enc_out_list = self.pdm_blocks[i](enc_out_list)\n",
    "\n",
    "        enc_out = enc_out_list[0]\n",
    "        # Output\n",
    "        # the output transformer encoder/decoder embeddings don't include non-linearity\n",
    "        output = self.act(enc_out)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        # (batch_size, seq_length * d_model)\n",
    "        output = output.reshape(output.shape[0], -1)\n",
    "        output = self.projection(output)  # (batch_size, num_classes)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def forward(self, x_enc):\n",
    "        dec_out = self.classification(x_enc)\n",
    "        return dec_out  # [B, N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "        'seq_len': 24,  # 입력 시퀀스 길이\n",
    "        'label_len': 1,  # 라벨 길이\n",
    "        'pred_len': 1,  # 예측할 길이\n",
    "        'down_sampling_window': 24,  # 다운샘플링 윈도우\n",
    "        'channel_independence': True,  # 채널 독립성 여부\n",
    "        'e_layers': 2,  # 인코더 레이어 수\n",
    "        'moving_avg': 3,  # 이동 평균\n",
    "        'enc_in': 532,  # 입력 차원\n",
    "        'd_model': 64,  # 모델의 차원\n",
    "        'd_ff' : 32,\n",
    "        'embed': 'fixed',  # 임베딩 방법\n",
    "        'decomp_method' : 'moving_avg',\n",
    "        'freq': 'h',  # 데이터의 주기 (예: 시간 단위)\n",
    "        'dropout': 0.1,  # 드롭아웃 비율\n",
    "        'num_class': 4,  # 출력 클래스 수\n",
    "        'use_norm': 1,  # 정규화 사용 여부\n",
    "        'down_sampling_layers': 2,  # 다운샘플링 레이어 수\n",
    "        'down_sampling_method': 'avg',  # 다운샘플링 방법 ('max', 'avg', 'conv' 중 선택)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data split for training\n",
    "train_data = train_df.iloc[:,2:]\n",
    "valid_data = train_df[\"target\"]\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_data, valid_data, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "# 3. 데이터 텐서 변환\n",
    "train_X_tensor = torch.tensor(x_train.values, dtype=torch.float32)\n",
    "train_y_tensor = torch.tensor(y_train.values.squeeze(), dtype=torch.long)\n",
    "valid_X_tensor = torch.tensor(x_valid.values, dtype=torch.float32)\n",
    "valid_y_tensor = torch.tensor(y_valid.values.squeeze(), dtype=torch.long)\n",
    "\n",
    "# 4. 데이터셋과 DataLoader 준비\n",
    "# seq_len 설정\n",
    "seq_len = 24\n",
    "batch_size = 64\n",
    "\n",
    "# 5. 시계열 데이터에 맞게 3차원 텐서로 변환\n",
    "def create_sequences(X, y, seq_len):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(X) - seq_len):\n",
    "        seq = X[i:i + seq_len]\n",
    "        label = y[i + seq_len]  # 다음 시간 스텝의 레이블\n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "    return torch.stack(sequences), torch.tensor(labels)\n",
    "\n",
    "train_X_seq, train_y_seq = create_sequences(train_X_tensor, train_y_tensor, seq_len)\n",
    "valid_X_seq, valid_y_seq = create_sequences(valid_X_tensor, valid_y_tensor, seq_len)\n",
    "\n",
    "# DataLoader 설정\n",
    "train_dataset = TensorDataset(train_X_seq, train_y_seq)\n",
    "valid_dataset = TensorDataset(valid_X_seq, valid_y_seq)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = Model(configs)\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (532x1x1). Calculated output size: (532x1x0). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[71], line 324\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x_enc)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_enc):\n\u001b[0;32m--> 324\u001b[0m     dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dec_out\n",
      "Cell \u001b[0;32mIn[71], line 298\u001b[0m, in \u001b[0;36mModel.classification\u001b[0;34m(self, x_enc)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclassification\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_enc):\n\u001b[0;32m--> 298\u001b[0m     x_enc, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__multi_scale_process_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     x_list \u001b[38;5;241m=\u001b[39m x_enc\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;66;03m# embedding\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[71], line 262\u001b[0m, in \u001b[0;36mModel.__multi_scale_process_inputs\u001b[0;34m(self, x_enc, x_mark_enc)\u001b[0m\n\u001b[1;32m    259\u001b[0m x_mark_sampling_list\u001b[38;5;241m.\u001b[39mappend(x_mark_enc)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfigs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdown_sampling_layers\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m--> 262\u001b[0m     x_enc_sampling \u001b[38;5;241m=\u001b[39m \u001b[43mdown_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc_ori\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m     x_enc_sampling_list\u001b[38;5;241m.\u001b[39mappend(x_enc_sampling\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    265\u001b[0m     x_enc_ori \u001b[38;5;241m=\u001b[39m x_enc_sampling\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/pooling.py:555\u001b[0m, in \u001b[0;36mAvgPool1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavg_pool1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount_include_pad\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given input size: (532x1x1). Calculated output size: (532x1x0). Output size is too small"
     ]
    }
   ],
   "source": [
    "model(next(iter(train_loader))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (532x1x1). Calculated output size: (532x1x0). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# Number of epochs to train\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 36\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m evaluate(model, valid_loader, device)\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[69], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m      5\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Zero the gradients\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)  \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[1;32m     10\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[51], line 325\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x_enc)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_enc):\n\u001b[0;32m--> 325\u001b[0m     dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dec_out\n",
      "Cell \u001b[0;32mIn[51], line 298\u001b[0m, in \u001b[0;36mModel.classification\u001b[0;34m(self, x_enc)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclassification\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_enc):\n\u001b[0;32m--> 298\u001b[0m     x_enc, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__multi_scale_process_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     x_list \u001b[38;5;241m=\u001b[39m x_enc\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;66;03m# embedding\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[51], line 262\u001b[0m, in \u001b[0;36mModel.__multi_scale_process_inputs\u001b[0;34m(self, x_enc, x_mark_enc)\u001b[0m\n\u001b[1;32m    259\u001b[0m x_mark_sampling_list\u001b[38;5;241m.\u001b[39mappend(x_mark_enc)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfigs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdown_sampling_layers\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m--> 262\u001b[0m     x_enc_sampling \u001b[38;5;241m=\u001b[39m \u001b[43mdown_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc_ori\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m     x_enc_sampling_list\u001b[38;5;241m.\u001b[39mappend(x_enc_sampling\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    265\u001b[0m     x_enc_ori \u001b[38;5;241m=\u001b[39m x_enc_sampling\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.11/site-packages/torch/nn/modules/pooling.py:555\u001b[0m, in \u001b[0;36mAvgPool1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavg_pool1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount_include_pad\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given input size: (532x1x1). Calculated output size: (532x1x0). Output size is too small"
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Calculate loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update parameters\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, valid_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Get the class with the highest score\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 10  # Number of epochs to train\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, train_loader, criterion, optimizer, device)\n",
    "    accuracy = evaluate(model, valid_loader, device)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Test Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
